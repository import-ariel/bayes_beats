---
layout: home
title: Methodology
permalink: /methodology/
---

# Methodology


# Fine-tuning ACE-Step
| **#** | **OOM Error Context**                                                                               | **What We Did**                                                                                                                                                                                                   | **Logic / Rationale**                                                                                                                                                      |
| ----- | --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1     | **Initial OOM during training; CUDA out of memory (needed \~1.25GB, 22GB total, only \~1GB free).** | Reduced batch size to 1; used mixed precision (`precision=16-mixed`); enabled gradient accumulation (`accumulate_grad_batches > 1`).                                                                              | Lowering batch size reduces per-step memory; mixed precision halves activation sizes; gradient accumulation lets you mimic larger batch sizes over multiple smaller steps. |
| 2     | **OOM in DCAE encoder’s forward pass (autocast and memory-saving tweaks still OOM).**               | Froze all large backbone models (`eval()` mode), only LoRA adapters trainable; made sure all non-LoRA modules had `.requires_grad_(False)`; checked that no “plot” or evaluation steps were running mid-training. | Training only LoRA drastically reduces memory since large weights are frozen; disabling evaluation steps prevents extra forward passes and memory spikes.                  |
| 3     | **OOM during validation or plotting (plot\_step, predict\_step, etc.)**                             | Commented out `self.plot_step()` and any validation/prediction steps that generate extra forward passes during training.                                                                                          | Plotting and eval can easily double memory use (forward pass through the model, storing intermediate results). Disabling these keeps only the essentials in RAM.           |
| 4     | **OOM persists at DCAE encoder during DataLoader or forward pass (batch size = 1).**                | Checked and minimized all input shapes (audio length, sample rate); ensured DataLoader is not stacking extra samples by mistake; considered reducing the chunk size for SSL feature extractors.                   | Even a single input can OOM if it’s large (e.g., long audio, high sample rate, large latent space). Smaller chunks reduce max memory spike in forward pass.                |
| 5     | **OOM in PyTorch/Colab despite above (e.g., at `self.dcae.encode`).**                               | Prepared to switch to more powerful hardware (A100 GPU); considered reducing dataset size and/or limiting training steps.                                                                                         | Sometimes, the only path forward is hardware with more VRAM or scaling down the data/model size for feasibility.                                                           |
| 6     | **PyTorch Fragmentation warning:**                                                                  | Set environment variable: `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`                                                                                                                                      | Helps reduce CUDA memory fragmentation, which can sometimes make more memory available for large tensors.                                                                  |
| 7     | **Attempted to set `--limit_val_batches 0.0` (argparse error).**                                    | Removed this argument, as your script’s CLI didn’t recognize it; focused instead on disabling validation with arguments it actually supports.                                                                     | Ensures only recognized CLI arguments are passed—prevents crash at startup, even if it doesn’t directly affect memory use.                                                 |
| 8     | **Still hitting OOM after all above.**                                                              | Plan to: 1) switch to A100 GPU; 2) further reduce input data size; 3) possibly reduce model complexity or chunk inputs further.                                                                                   | When software optimizations plateau, better hardware or smaller jobs are the final options.                                                                                |

