{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JrxO6R7117O"
      },
      "source": [
        "# Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLdpUtmMCCfl"
      },
      "outputs": [],
      "source": [
        "# -- Install as needed --\n",
        "\n",
        "# !pip install librosa soundfile\n",
        "# !pip install sentence-transformers\n",
        "# !pip install google-cloud-storage\n",
        "# !pip install openai\n",
        "# !pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB6CDxq5BLl7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "#from openai import OpenAI\n",
        "from google.cloud import storage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import tempfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ienztjVyDYsK",
        "outputId": "019ea579-bb99-4e11-e549-d398472b3313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê Authenticating with Google Cloud...\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for authentication\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "import os\n",
        "\n",
        "# Authenticate with Google Cloud\n",
        "print(\"üîê Authenticating with Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up your specific Google Cloud projectr project ID from the screenshot\n",
        "project_id = \"bayes-beats\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "d2nXyYC8AYn0",
        "outputId": "7ba27e65-41b8-4ce7-b64b-ed7c7434f678"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Error connecting to GCS: 403 GET https://storage.googleapis.com/storage/v1/b/uchicago-bayesian-bayes-beats?projection=noAcl&prettyPrint=false: drosenthal@uchicago.edu does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).\n",
            "Make sure you've run the setup script and authenticated properly\n"
          ]
        },
        {
          "ename": "Forbidden",
          "evalue": "403 GET https://storage.googleapis.com/storage/v1/b/uchicago-bayesian-bayes-beats?projection=noAcl&prettyPrint=false: drosenthal@uchicago.edu does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-010315a0e1e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUCKET_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Test bucket access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Successfully connected to bucket: {BUCKET_NAME}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/contextlib.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/bucket.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self, client, projection, timeout, if_etag_match, if_etag_not_match, if_metageneration_match, if_metageneration_not_match, retry, soft_deleted)\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mSee\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcom\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msoft\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \"\"\"\n\u001b[0;32m-> 1147\u001b[0;31m         super(Bucket, self).reload(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             \u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/_helpers.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self, client, projection, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, retry, soft_deleted)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_etag_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_etag_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_etag_not_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mif_etag_not_match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         )\n\u001b[0;32m--> 303\u001b[0;31m         api_response = client._get_resource(\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/client.py\u001b[0m in \u001b[0;36m_get_resource\u001b[0;34m(self, path, query_params, headers, timeout, retry, _target_object)\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbucket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \"\"\"\n\u001b[0;32m--> 474\u001b[0;31m         return self._connection.api_request(\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/storage/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mForbidden\u001b[0m: 403 GET https://storage.googleapis.com/storage/v1/b/uchicago-bayesian-bayes-beats?projection=noAcl&prettyPrint=false: drosenthal@uchicago.edu does not have storage.buckets.get access to the Google Cloud Storage bucket. Permission 'storage.buckets.get' denied on resource (or it may not exist)."
          ]
        }
      ],
      "source": [
        "#from google.cloud import storage\n",
        "#from tqdm import tqdm\n",
        "\n",
        "\n",
        "# --- Config ---\n",
        "BUCKET_NAME = \"uchicago-bayesian-bayes-beats\"\n",
        "BASE_FOLDER = \"jamendo_by_genre\"\n",
        "CLIENT_ID = \"a62fe3ee\"\n",
        "AUDIO_FORMAT = \"mp32\"\n",
        "TRACKS_PER_GENRE = 20\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 30\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = 512\n",
        "\n",
        "\n",
        "# --- Initialize clients and models ---\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "try:\n",
        "    # Try to get project ID from environment\n",
        "    project_id = os.environ.get('GOOGLE_CLOUD_PROJECT')\n",
        "    if project_id:\n",
        "        client = storage.Client(project=project_id)\n",
        "    else:\n",
        "        client = storage.Client()  # Will use default project\n",
        "\n",
        "    bucket = client.bucket(BUCKET_NAME)\n",
        "    # Test bucket access\n",
        "    bucket.reload()\n",
        "    print(f\"‚úÖ Successfully connected to bucket: {BUCKET_NAME}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error connecting to GCS: {e}\")\n",
        "    print(\"Make sure you've run the setup script and authenticated properly\")\n",
        "    raise\n",
        "\n",
        "\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZbhB9dNBJ8e"
      },
      "outputs": [],
      "source": [
        "# OpenAI key\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Cg0IPmXjFwnJ",
        "outputId": "6ae9d394-ac1b-45a4-8763-9907ca49256a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéµ Starting Music Vibe Processing Pipeline...\n",
            "Initializing models...\n",
            "Discovering genres...\n",
            "Found ['reggae', 'electronic', 'ambient', 'blues', 'relaxation', 'metal', 'funk', 'rock', 'soundtrack', 'jazz', 'classical', 'hiphop', 'world', 'pop', 'latin']:\n",
            "  1. reggae\n",
            "  2. electronic\n",
            "  3. ambient\n",
            "  4. blues\n",
            "  5. relaxation\n",
            "  6. metal\n",
            "  7. funk\n",
            "  8. rock\n",
            "  9. soundtrack\n",
            "  10. jazz\n",
            "  11. classical\n",
            "  12. hiphop\n",
            "  13. world\n",
            "  14. pop\n",
            "  15. latin\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing genres:   0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üé∂ Processing genre: reggae\n",
            "Processing 20 songs from reggae\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing reggae songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing reggae songs:   5%|‚ñå         | 1/20 [00:28<08:59, 28.41s/it]\u001b[A\n",
            "Processing reggae songs:  10%|‚ñà         | 2/20 [00:29<03:43, 12.40s/it]\u001b[A\n",
            "Processing reggae songs:  15%|‚ñà‚ñå        | 3/20 [00:30<02:03,  7.27s/it]\u001b[A\n",
            "Processing reggae songs:  20%|‚ñà‚ñà        | 4/20 [00:31<01:17,  4.84s/it]\u001b[A\n",
            "Processing reggae songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:32<00:51,  3.46s/it]\u001b[A\n",
            "Processing reggae songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:34<00:37,  2.71s/it]\u001b[A\n",
            "Processing reggae songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:36<00:33,  2.54s/it]\u001b[A\n",
            "Processing reggae songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:37<00:25,  2.09s/it]\u001b[A\n",
            "Processing reggae songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:38<00:19,  1.79s/it]\u001b[A\n",
            "Processing reggae songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:39<00:15,  1.56s/it]\u001b[A\n",
            "Processing reggae songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:40<00:12,  1.40s/it]\u001b[A\n",
            "Processing reggae songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:41<00:10,  1.33s/it]\u001b[A\n",
            "Processing reggae songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:42<00:08,  1.23s/it]\u001b[A\n",
            "Processing reggae songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:43<00:07,  1.20s/it]\u001b[A\n",
            "Processing reggae songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:45<00:05,  1.19s/it]\u001b[A\n",
            "Processing reggae songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:46<00:04,  1.18s/it]\u001b[A\n",
            "Processing reggae songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:48<00:04,  1.46s/it]\u001b[A\n",
            "Processing reggae songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:49<00:02,  1.34s/it]\u001b[A\n",
            "Processing reggae songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:50<00:01,  1.30s/it]\u001b[A\n",
            "Processing reggae songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:51<00:00,  1.24s/it]\u001b[A\n",
            "Processing genres:   7%|‚ñã         | 1/15 [00:51<12:06, 51.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from reggae\n",
            "\n",
            "üé∂ Processing genre: electronic\n",
            "Processing 20 songs from electronic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing electronic songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing electronic songs:   5%|‚ñå         | 1/20 [00:01<00:26,  1.37s/it]\u001b[A\n",
            "Processing electronic songs:  10%|‚ñà         | 2/20 [00:02<00:21,  1.17s/it]\u001b[A\n",
            "Processing electronic songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:19,  1.14s/it]\u001b[A\n",
            "Processing electronic songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:19,  1.20s/it]\u001b[A\n",
            "Processing electronic songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:17,  1.17s/it]\u001b[A\n",
            "Processing electronic songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:07<00:16,  1.15s/it]\u001b[A\n",
            "Processing electronic songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:08<00:15,  1.18s/it]\u001b[A\n",
            "Processing electronic songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:09<00:13,  1.15s/it]\u001b[A\n",
            "Processing electronic songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:12,  1.14s/it]\u001b[A\n",
            "Processing electronic songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:11<00:10,  1.09s/it]\u001b[A\n",
            "Processing electronic songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:09,  1.09s/it]\u001b[A\n",
            "Processing electronic songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:13<00:08,  1.08s/it]\u001b[A\n",
            "Processing electronic songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:14<00:07,  1.09s/it]\u001b[A\n",
            "Processing electronic songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:15<00:06,  1.09s/it]\u001b[A\n",
            "Processing electronic songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:17<00:05,  1.15s/it]\u001b[A\n",
            "Processing electronic songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:18<00:04,  1.15s/it]\u001b[A\n",
            "Processing electronic songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:19<00:03,  1.16s/it]\u001b[A\n",
            "Processing electronic songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:20<00:02,  1.27s/it]\u001b[A\n",
            "Processing electronic songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:22<00:01,  1.22s/it]\u001b[A\n",
            "Processing electronic songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:23<00:00,  1.18s/it]\u001b[A\n",
            "Processing genres:  13%|‚ñà‚ñé        | 2/15 [01:15<07:35, 35.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from electronic\n",
            "\n",
            "üé∂ Processing genre: ambient\n",
            "Warning: Only 10 songs available in ambient, sampling all\n",
            "Processing 10 songs from ambient\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing ambient songs:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            "Processing ambient songs:  10%|‚ñà         | 1/10 [00:01<00:09,  1.06s/it]\u001b[A\n",
            "Processing ambient songs:  20%|‚ñà‚ñà        | 2/10 [00:02<00:09,  1.23s/it]\u001b[A\n",
            "Processing ambient songs:  30%|‚ñà‚ñà‚ñà       | 3/10 [00:03<00:08,  1.15s/it]\u001b[A\n",
            "Processing ambient songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:04<00:06,  1.12s/it]\u001b[A\n",
            "Processing ambient songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:10<00:13,  2.79s/it]\u001b[A\n",
            "Processing ambient songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [00:11<00:08,  2.20s/it]\u001b[A\n",
            "Processing ambient songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [00:12<00:05,  1.83s/it]\u001b[A\n",
            "Processing ambient songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [00:13<00:03,  1.60s/it]\u001b[A\n",
            "Processing ambient songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [00:14<00:01,  1.48s/it]\u001b[A\n",
            "Processing ambient songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:15<00:00,  1.41s/it]\u001b[A\n",
            "Processing genres:  20%|‚ñà‚ñà        | 3/15 [01:31<05:16, 26.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 10 songs from ambient\n",
            "\n",
            "üé∂ Processing genre: blues\n",
            "Processing 20 songs from blues\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing blues songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing blues songs:   5%|‚ñå         | 1/20 [00:01<00:23,  1.23s/it]\u001b[A\n",
            "Processing blues songs:  10%|‚ñà         | 2/20 [00:02<00:21,  1.17s/it]\u001b[A\n",
            "Processing blues songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:18,  1.11s/it]\u001b[A\n",
            "Processing blues songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:16,  1.05s/it]\u001b[A\n",
            "Processing blues songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:15,  1.03s/it]\u001b[A\n",
            "Processing blues songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:06<00:14,  1.05s/it]\u001b[A\n",
            "Processing blues songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:07<00:13,  1.03s/it]\u001b[A\n",
            "Processing blues songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:08<00:12,  1.02s/it]\u001b[A\n",
            "Processing blues songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:09<00:12,  1.10s/it]\u001b[A\n",
            "Processing blues songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:10<00:11,  1.13s/it]\u001b[A\n",
            "Processing blues songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:10,  1.13s/it]\u001b[A\n",
            "Processing blues songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:13<00:09,  1.19s/it]\u001b[A\n",
            "Processing blues songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:14<00:08,  1.15s/it]\u001b[A\n",
            "Processing blues songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:16<00:07,  1.31s/it]\u001b[A\n",
            "Processing blues songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:17<00:06,  1.23s/it]\u001b[A\n",
            "Processing blues songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:18<00:04,  1.19s/it]\u001b[A\n",
            "Processing blues songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:19<00:03,  1.14s/it]\u001b[A\n",
            "Processing blues songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:20<00:02,  1.12s/it]\u001b[A\n",
            "Processing blues songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:21<00:01,  1.15s/it]\u001b[A\n",
            "Processing blues songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.02s/it]\u001b[A\n",
            "Processing genres:  27%|‚ñà‚ñà‚ñã       | 4/15 [01:53<04:32, 24.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from blues\n",
            "\n",
            "üé∂ Processing genre: relaxation\n",
            "Processing 20 songs from relaxation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing relaxation songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing relaxation songs:   5%|‚ñå         | 1/20 [00:01<00:19,  1.02s/it]\u001b[A\n",
            "Processing relaxation songs:  10%|‚ñà         | 2/20 [00:02<00:18,  1.05s/it]\u001b[A\n",
            "Processing relaxation songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:19,  1.12s/it]\u001b[A\n",
            "Processing relaxation songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:17,  1.07s/it]\u001b[A\n",
            "Processing relaxation songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:15,  1.07s/it]\u001b[A\n",
            "Processing relaxation songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:06<00:14,  1.06s/it]\u001b[A\n",
            "Processing relaxation songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:07<00:13,  1.06s/it]\u001b[A\n",
            "Processing relaxation songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:08<00:13,  1.13s/it]\u001b[A\n",
            "Processing relaxation songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:09<00:12,  1.09s/it]\u001b[A\n",
            "Processing relaxation songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:10<00:10,  1.06s/it]\u001b[A\n",
            "Processing relaxation songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:11<00:09,  1.03s/it]\u001b[A\n",
            "Processing relaxation songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:12<00:08,  1.05s/it]\u001b[A\n",
            "Processing relaxation songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:13<00:07,  1.06s/it]\u001b[A\n",
            "Processing relaxation songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:15<00:06,  1.12s/it]\u001b[A\n",
            "Processing relaxation songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:16<00:05,  1.10s/it]\u001b[A\n",
            "Processing relaxation songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:17<00:04,  1.13s/it]\u001b[A\n",
            "Processing relaxation songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:18<00:03,  1.14s/it]\u001b[A\n",
            "Processing relaxation songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:19<00:02,  1.14s/it]\u001b[A\n",
            "Processing relaxation songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:20<00:01,  1.10s/it]\u001b[A\n",
            "Processing relaxation songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:21<00:00,  1.09s/it]\u001b[A\n",
            "Processing genres:  33%|‚ñà‚ñà‚ñà‚ñé      | 5/15 [02:15<03:56, 23.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from relaxation\n",
            "\n",
            "üé∂ Processing genre: metal\n",
            "Processing 20 songs from metal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing metal songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing metal songs:   5%|‚ñå         | 1/20 [00:01<00:22,  1.16s/it]\u001b[A\n",
            "Processing metal songs:  10%|‚ñà         | 2/20 [00:02<00:20,  1.14s/it]\u001b[A\n",
            "Processing metal songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:20,  1.21s/it]\u001b[A\n",
            "Processing metal songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:19,  1.20s/it]\u001b[A\n",
            "Processing metal songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:17,  1.17s/it]\u001b[A\n",
            "Processing metal songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:08<00:21,  1.55s/it]\u001b[A\n",
            "Processing metal songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:09<00:18,  1.42s/it]\u001b[A\n",
            "Processing metal songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:10<00:15,  1.33s/it]\u001b[A\n",
            "Processing metal songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:11<00:13,  1.25s/it]\u001b[A\n",
            "Processing metal songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:12<00:12,  1.22s/it]\u001b[A\n",
            "Processing metal songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:13<00:10,  1.17s/it]\u001b[A\n",
            "Processing metal songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:14<00:09,  1.14s/it]\u001b[A\n",
            "Processing metal songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:15<00:08,  1.15s/it]\u001b[A\n",
            "Processing metal songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:17<00:06,  1.14s/it]\u001b[A\n",
            "Processing metal songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:18<00:05,  1.11s/it]\u001b[A\n",
            "Processing metal songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:22<00:08,  2.07s/it]\u001b[A\n",
            "Processing metal songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:23<00:05,  1.81s/it]\u001b[A\n",
            "Processing metal songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:24<00:03,  1.60s/it]\u001b[A\n",
            "Processing metal songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:26<00:01,  1.56s/it]\u001b[A\n",
            "Processing metal songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:27<00:00,  1.41s/it]\u001b[A\n",
            "Processing genres:  40%|‚ñà‚ñà‚ñà‚ñà      | 6/15 [02:42<03:44, 24.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from metal\n",
            "\n",
            "üé∂ Processing genre: funk\n",
            "Processing 20 songs from funk\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing funk songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing funk songs:   5%|‚ñå         | 1/20 [00:01<00:24,  1.28s/it]\u001b[A\n",
            "Processing funk songs:  10%|‚ñà         | 2/20 [00:02<00:20,  1.16s/it]\u001b[A\n",
            "Processing funk songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:18,  1.07s/it]\u001b[A\n",
            "Processing funk songs:  20%|‚ñà‚ñà        | 4/20 [00:06<00:28,  1.75s/it]\u001b[A\n",
            "Processing funk songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:07<00:22,  1.52s/it]\u001b[A\n",
            "Processing funk songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:10<00:28,  2.04s/it]\u001b[A\n",
            "Processing funk songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:11<00:22,  1.74s/it]\u001b[A\n",
            "Processing funk songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:12<00:19,  1.59s/it]\u001b[A\n",
            "Processing funk songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:13<00:15,  1.42s/it]\u001b[A\n",
            "Processing funk songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:14<00:13,  1.35s/it]\u001b[A\n",
            "Processing funk songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:15<00:11,  1.23s/it]\u001b[A\n",
            "Processing funk songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:16<00:09,  1.15s/it]\u001b[A\n",
            "Processing funk songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:17<00:07,  1.13s/it]\u001b[A\n",
            "Processing funk songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:19<00:06,  1.13s/it]\u001b[A\n",
            "Processing funk songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:20<00:05,  1.13s/it]\u001b[A\n",
            "Processing funk songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:21<00:04,  1.10s/it]\u001b[A\n",
            "Processing funk songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:22<00:03,  1.06s/it]\u001b[A\n",
            "Processing funk songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:23<00:02,  1.04s/it]\u001b[A\n",
            "Processing funk songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:24<00:01,  1.13s/it]\u001b[A\n",
            "Processing funk songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:25<00:00,  1.11s/it]\u001b[A\n",
            "Processing genres:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 7/15 [03:08<03:21, 25.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from funk\n",
            "\n",
            "üé∂ Processing genre: rock\n",
            "Processing 20 songs from rock\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing rock songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing rock songs:   5%|‚ñå         | 1/20 [00:01<00:19,  1.02s/it]\u001b[A\n",
            "Processing rock songs:  10%|‚ñà         | 2/20 [00:02<00:18,  1.01s/it]\u001b[A\n",
            "Processing rock songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:17,  1.05s/it]\u001b[A\n",
            "Processing rock songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:17,  1.07s/it]\u001b[A\n",
            "Processing rock songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:16,  1.07s/it]\u001b[A\n",
            "Processing rock songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:06<00:14,  1.06s/it]\u001b[A\n",
            "Processing rock songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:07<00:15,  1.16s/it]\u001b[A\n",
            "Processing rock songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:08<00:14,  1.17s/it]\u001b[A\n",
            "Processing rock songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:13,  1.21s/it]\u001b[A\n",
            "Processing rock songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:11<00:11,  1.18s/it]\u001b[A\n",
            "Processing rock songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:10,  1.17s/it]\u001b[A\n",
            "Processing rock songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:13<00:08,  1.10s/it]\u001b[A\n",
            "Processing rock songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:14<00:07,  1.10s/it]\u001b[A\n",
            "Processing rock songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:15<00:06,  1.09s/it]\u001b[A\n",
            "Processing rock songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:16<00:05,  1.04s/it]\u001b[A\n",
            "Processing rock songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:17<00:04,  1.05s/it]\u001b[A\n",
            "Processing rock songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:18<00:03,  1.05s/it]\u001b[A\n",
            "Processing rock songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:19<00:02,  1.04s/it]\u001b[A\n",
            "Processing rock songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:20<00:01,  1.08s/it]\u001b[A\n",
            "Processing rock songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.19s/it]\u001b[A\n",
            "Processing genres:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 8/15 [03:30<02:49, 24.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from rock\n",
            "\n",
            "üé∂ Processing genre: soundtrack\n",
            "Processing 20 songs from soundtrack\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing soundtrack songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing soundtrack songs:   5%|‚ñå         | 1/20 [00:01<00:31,  1.67s/it]\u001b[A\n",
            "Processing soundtrack songs:  10%|‚ñà         | 2/20 [00:02<00:23,  1.30s/it]\u001b[A\n",
            "Processing soundtrack songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:20,  1.22s/it]\u001b[A\n",
            "Processing soundtrack songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:18,  1.15s/it]\u001b[A\n",
            "Processing soundtrack songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:16,  1.09s/it]\u001b[A\n",
            "Processing soundtrack songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:06<00:14,  1.06s/it]\u001b[A\n",
            "Processing soundtrack songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:07<00:14,  1.08s/it]\u001b[A\n",
            "Processing soundtrack songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:09<00:13,  1.13s/it]\u001b[A\n",
            "Processing soundtrack songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:11,  1.08s/it]\u001b[A\n",
            "Processing soundtrack songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:11<00:11,  1.14s/it]\u001b[A\n",
            "Processing soundtrack songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:09,  1.08s/it]\u001b[A\n",
            "Processing soundtrack songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:13<00:08,  1.04s/it]\u001b[A\n",
            "Processing soundtrack songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:14<00:07,  1.02s/it]\u001b[A\n",
            "Processing soundtrack songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:15<00:06,  1.05s/it]\u001b[A\n",
            "Processing soundtrack songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:16<00:05,  1.02s/it]\u001b[A\n",
            "Processing soundtrack songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:17<00:04,  1.09s/it]\u001b[A\n",
            "Processing soundtrack songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:18<00:03,  1.11s/it]\u001b[A\n",
            "Processing soundtrack songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:20<00:02,  1.18s/it]\u001b[A\n",
            "Processing soundtrack songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:21<00:01,  1.13s/it]\u001b[A\n",
            "Processing soundtrack songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.22s/it]\u001b[A\n",
            "Processing genres:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 9/15 [03:53<02:22, 23.74s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from soundtrack\n",
            "\n",
            "üé∂ Processing genre: jazz\n",
            "Processing 20 songs from jazz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing jazz songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing jazz songs:   5%|‚ñå         | 1/20 [00:01<00:21,  1.11s/it]\u001b[A\n",
            "Processing jazz songs:  10%|‚ñà         | 2/20 [00:02<00:20,  1.16s/it]\u001b[A\n",
            "Processing jazz songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:19,  1.16s/it]\u001b[A\n",
            "Processing jazz songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:17,  1.11s/it]\u001b[A\n",
            "Processing jazz songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:16,  1.08s/it]\u001b[A\n",
            "Processing jazz songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:06<00:15,  1.09s/it]\u001b[A\n",
            "Processing jazz songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:08<00:15,  1.22s/it]\u001b[A\n",
            "Processing jazz songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:09<00:14,  1.17s/it]\u001b[A\n",
            "Processing jazz songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:12,  1.17s/it]\u001b[A\n",
            "Processing jazz songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:11<00:11,  1.15s/it]\u001b[A\n",
            "Processing jazz songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:10,  1.19s/it]\u001b[A\n",
            "Processing jazz songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:13<00:09,  1.15s/it]\u001b[A\n",
            "Processing jazz songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:14<00:08,  1.17s/it]\u001b[A\n",
            "Processing jazz songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:16<00:06,  1.13s/it]\u001b[A\n",
            "Processing jazz songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:17<00:05,  1.10s/it]\u001b[A\n",
            "Processing jazz songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:18<00:04,  1.07s/it]\u001b[A\n",
            "Processing jazz songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:19<00:03,  1.07s/it]\u001b[A\n",
            "Processing jazz songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:20<00:02,  1.06s/it]\u001b[A\n",
            "Processing jazz songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:21<00:01,  1.06s/it]\u001b[A\n",
            "Processing jazz songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.06s/it]\u001b[A\n",
            "Processing genres:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 10/15 [04:15<01:56, 23.31s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from jazz\n",
            "\n",
            "üé∂ Processing genre: classical\n",
            "Processing 20 songs from classical\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing classical songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing classical songs:   5%|‚ñå         | 1/20 [00:01<00:23,  1.23s/it]\u001b[A\n",
            "Processing classical songs:  10%|‚ñà         | 2/20 [00:02<00:22,  1.23s/it]\u001b[A\n",
            "Processing classical songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:21,  1.25s/it]\u001b[A\n",
            "Processing classical songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:19,  1.20s/it]\u001b[A\n",
            "Processing classical songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:17,  1.16s/it]\u001b[A\n",
            "Processing classical songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:07<00:15,  1.14s/it]\u001b[A\n",
            "Processing classical songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:08<00:14,  1.09s/it]\u001b[A\n",
            "Processing classical songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:08<00:12,  1.04s/it]\u001b[A\n",
            "Processing classical songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:11,  1.05s/it]\u001b[A\n",
            "Processing classical songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:11<00:10,  1.10s/it]\u001b[A\n",
            "Processing classical songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:09,  1.05s/it]\u001b[A\n",
            "Processing classical songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:14<00:11,  1.45s/it]\u001b[A\n",
            "Processing classical songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:15<00:09,  1.29s/it]\u001b[A\n",
            "Processing classical songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:16<00:07,  1.23s/it]\u001b[A\n",
            "Processing classical songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:17<00:06,  1.27s/it]\u001b[A\n",
            "Processing classical songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:18<00:04,  1.20s/it]\u001b[A\n",
            "Processing classical songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:20<00:03,  1.17s/it]\u001b[A\n",
            "Processing classical songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:21<00:02,  1.30s/it]\u001b[A\n",
            "Processing classical songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:23<00:01,  1.31s/it]\u001b[A\n",
            "Processing classical songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:24<00:00,  1.32s/it]\u001b[A\n",
            "Processing genres:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 11/15 [04:39<01:34, 23.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from classical\n",
            "\n",
            "üé∂ Processing genre: hiphop\n",
            "Processing 20 songs from hiphop\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing hiphop songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing hiphop songs:   5%|‚ñå         | 1/20 [00:01<00:24,  1.31s/it]\u001b[A\n",
            "Processing hiphop songs:  10%|‚ñà         | 2/20 [00:02<00:20,  1.14s/it]\u001b[A\n",
            "Processing hiphop songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:19,  1.15s/it]\u001b[A\n",
            "Processing hiphop songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:17,  1.09s/it]\u001b[A\n",
            "Processing hiphop songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:05<00:15,  1.04s/it]\u001b[A\n",
            "Processing hiphop songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:06<00:15,  1.07s/it]\u001b[A\n",
            "Processing hiphop songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:07<00:15,  1.16s/it]\u001b[A\n",
            "Processing hiphop songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:09<00:13,  1.17s/it]\u001b[A\n",
            "Processing hiphop songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:12,  1.16s/it]\u001b[A\n",
            "Processing hiphop songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:11<00:12,  1.21s/it]\u001b[A\n",
            "Processing hiphop songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:12<00:11,  1.26s/it]\u001b[A\n",
            "Processing hiphop songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:14<00:11,  1.38s/it]\u001b[A\n",
            "Processing hiphop songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:15<00:09,  1.29s/it]\u001b[A\n",
            "Processing hiphop songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:16<00:07,  1.22s/it]\u001b[A\n",
            "Processing hiphop songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:17<00:05,  1.19s/it]\u001b[A\n",
            "Processing hiphop songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:18<00:04,  1.13s/it]\u001b[A\n",
            "Processing hiphop songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:20<00:03,  1.17s/it]\u001b[A\n",
            "Processing hiphop songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:21<00:02,  1.21s/it]\u001b[A\n",
            "Processing hiphop songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:22<00:01,  1.20s/it]\u001b[A\n",
            "Processing hiphop songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:23<00:00,  1.14s/it]\u001b[A\n",
            "Processing genres:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 12/15 [05:03<01:10, 23.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from hiphop\n",
            "\n",
            "üé∂ Processing genre: world\n",
            "Processing 20 songs from world\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing world songs:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
            "Processing world songs:   5%|‚ñå         | 1/20 [00:01<00:21,  1.11s/it]\u001b[A\n",
            "Processing world songs:  10%|‚ñà         | 2/20 [00:02<00:18,  1.04s/it]\u001b[A\n",
            "Processing world songs:  15%|‚ñà‚ñå        | 3/20 [00:03<00:17,  1.03s/it]\u001b[A\n",
            "Processing world songs:  20%|‚ñà‚ñà        | 4/20 [00:04<00:18,  1.16s/it]\u001b[A\n",
            "Processing world songs:  25%|‚ñà‚ñà‚ñå       | 5/20 [00:06<00:19,  1.32s/it]\u001b[A\n",
            "Processing world songs:  30%|‚ñà‚ñà‚ñà       | 6/20 [00:07<00:17,  1.26s/it]\u001b[A\n",
            "Processing world songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [00:08<00:15,  1.22s/it]\u001b[A\n",
            "Processing world songs:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [00:09<00:14,  1.22s/it]\u001b[A\n",
            "Processing world songs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [00:10<00:13,  1.21s/it]\u001b[A\n",
            "Processing world songs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [00:12<00:13,  1.31s/it]\u001b[A\n",
            "Processing world songs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [00:13<00:11,  1.29s/it]\u001b[A\n",
            "Processing world songs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [00:14<00:10,  1.28s/it]\u001b[A\n",
            "Processing world songs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [00:15<00:08,  1.19s/it]\u001b[A\n",
            "Processing world songs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [00:17<00:07,  1.29s/it]\u001b[A\n",
            "Processing world songs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [00:18<00:06,  1.32s/it]\u001b[A\n",
            "Processing world songs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [00:20<00:05,  1.40s/it]\u001b[A\n",
            "Processing world songs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [00:21<00:04,  1.39s/it]\u001b[A\n",
            "Processing world songs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [00:22<00:02,  1.34s/it]\u001b[A\n",
            "Processing world songs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [00:24<00:01,  1.37s/it]\u001b[A\n",
            "Processing world songs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:25<00:00,  1.40s/it]\u001b[A\n",
            "Processing genres:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 13/15 [05:29<00:48, 24.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed 20 songs from world\n",
            "\n",
            "üé∂ Processing genre: pop\n",
            "Warning: Only 17 songs available in pop, sampling all\n",
            "Processing 17 songs from pop\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing pop songs:   0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Processing pop songs:   6%|‚ñå         | 1/17 [00:01<00:19,  1.22s/it]\u001b[A\n",
            "Processing pop songs:  12%|‚ñà‚ñè        | 2/17 [00:02<00:17,  1.16s/it]\u001b[A\n",
            "Processing pop songs:  18%|‚ñà‚ñä        | 3/17 [00:03<00:15,  1.13s/it]\u001b[A\n",
            "Processing pop songs:  24%|‚ñà‚ñà‚ñé       | 4/17 [00:04<00:15,  1.19s/it]\u001b[A\n",
            "Processing pop songs:  29%|‚ñà‚ñà‚ñâ       | 5/17 [00:05<00:13,  1.16s/it]\u001b[A\n",
            "Processing pop songs:  35%|‚ñà‚ñà‚ñà‚ñå      | 6/17 [00:06<00:12,  1.13s/it]\u001b[A\n",
            "Processing genres:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 13/15 [05:36<00:51, 25.90s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b1d12d7507d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;31m#     print(f\"Found existing processed data with {len(saved_data)} samples\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;31m# Example of using the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b1d12d7507d6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mgenre_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_blobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Processing {genre} songs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_audio_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcae_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0mgenre_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b1d12d7507d6>\u001b[0m in \u001b[0;36mprocess_audio_file\u001b[0;34m(blob, genre, feature_extractor, dcae_model)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Extract audio features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-b1d12d7507d6>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, audio_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Chroma features (harmonic content)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mchroma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchroma_stft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Zero crossing rate (indicates percussive vs harmonic content)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mchroma_stft\u001b[0;34m(y, sr, S, norm, n_fft, hop_length, win_length, window, center, pad_mode, tuning, n_chroma, **kwargs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtuning\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mtuning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins_per_octave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_chroma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;31m# Get the filter bank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py\u001b[0m in \u001b[0;36mestimate_tuning\u001b[0;34m(y, sr, S, n_fft, resolution, bins_per_octave, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;36m0.08000000000000002\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mpitch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpiptrack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Only count magnitude where frequency is > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py\u001b[0m in \u001b[0;36mpiptrack\u001b[0;34m(y, sr, S, n_fft, hop_length, fmin, fmax, threshold, win_length, window, center, pad_mode, ref)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# then restrict to the feasible range (fmin:fmax)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mshift\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parabolic_interpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0;31m# this will get us the interpolated peak value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0mdskew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py\u001b[0m in \u001b[0;36m_parabolic_interpolation\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;31m# Call the vectorized stencil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0m_pi_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshiftsi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;31m# Handle the edge condition not covered by the stencil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba/np/ufunc/gufunc.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# call the underlying gufunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frozen\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dynamic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"out\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# If \"out\" argument is supplied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class AudioFeatureExtractor:\n",
        "    \"\"\"Extract comprehensive audio features for vibe analysis\"\"\"\n",
        "\n",
        "    def __init__(self, sr=SAMPLE_RATE, duration=DURATION):\n",
        "        self.sr = sr\n",
        "        self.duration = duration\n",
        "\n",
        "    def extract_features(self, audio_path):\n",
        "        \"\"\"Extract audio features including tempo, spectral features, and rhythm\"\"\"\n",
        "        try:\n",
        "            # Load audio\n",
        "            y, sr = librosa.load(audio_path, sr=self.sr, duration=self.duration)\n",
        "\n",
        "            # Basic features\n",
        "            tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "            # Spectral features\n",
        "            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
        "            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
        "            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
        "\n",
        "            # MFCC features\n",
        "            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "            # Chroma features (harmonic content)\n",
        "            chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "\n",
        "            # Zero crossing rate (indicates percussive vs harmonic content)\n",
        "            zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
        "\n",
        "            # RMS Energy\n",
        "            rms = librosa.feature.rms(y=y)[0]\n",
        "\n",
        "            # Mel spectrogram for deep learning\n",
        "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
        "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "            features = {\n",
        "                'tempo': float(tempo),\n",
        "                'spectral_centroid_mean': float(np.mean(spectral_centroids)),\n",
        "                'spectral_centroid_std': float(np.std(spectral_centroids)),\n",
        "                'spectral_rolloff_mean': float(np.mean(spectral_rolloff)),\n",
        "                'spectral_bandwidth_mean': float(np.mean(spectral_bandwidth)),\n",
        "                'mfcc_mean': np.mean(mfccs, axis=1).tolist(),\n",
        "                'chroma_mean': np.mean(chroma, axis=1).tolist(),\n",
        "                'zcr_mean': float(np.mean(zcr)),\n",
        "                'rms_mean': float(np.mean(rms)),\n",
        "                'rms_std': float(np.std(rms)),\n",
        "                'mel_spectrogram': mel_spec_db.tolist()  # For DCAE input\n",
        "            }\n",
        "\n",
        "            return features\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting features: {e}\")\n",
        "            return None\n",
        "\n",
        "class SimpleDCAE(nn.Module):\n",
        "    \"\"\"Simplified Deep Convolutional Autoencoder for audio latent representation\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape=(N_MELS, 1292)):  # Approximate mel-spec shape for 30s audio\n",
        "        super(SimpleDCAE, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((8, 8)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 8 * 8, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128)  # Latent dimension\n",
        "        )\n",
        "\n",
        "        # Decoder (for training, not used in final pipeline)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(128, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 128 * 8 * 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Unflatten(1, (128, 8, 8)),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encode(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "def generate_vibe_description(features, genre, filename):\n",
        "    \"\"\"Generate vibe description using OpenAI GPT\"\"\"\n",
        "\n",
        "    # Create a descriptive prompt based on audio features\n",
        "    tempo_desc = \"fast-paced\" if features['tempo'] > 120 else \"moderate\" if features['tempo'] > 80 else \"slow\"\n",
        "    energy_desc = \"high-energy\" if features['rms_mean'] > 0.1 else \"mellow\"\n",
        "    brightness_desc = \"bright\" if features['spectral_centroid_mean'] > 2000 else \"warm\"\n",
        "\n",
        "    system_prompt = \"\"\"You are a music curator who creates evocative, atmospheric descriptions of songs based on their audio characteristics.\n",
        "Create a short, vivid description that captures the 'vibe' or mood the song would create.\n",
        "Focus on emotions, settings, activities, or scenarios the music evokes.\n",
        "Keep it under 20 words and make it atmospheric and engaging.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Genre: {genre}\n",
        "Tempo: {features['tempo']:.1f} BPM ({tempo_desc})\n",
        "Energy: {energy_desc}\n",
        "Tonal quality: {brightness_desc}\n",
        "Zero-crossing rate: {features['zcr_mean']:.4f}\n",
        "\n",
        "Create a vibe description for this {genre} track.\"\"\"\n",
        "\n",
        "    try:\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\",   \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=50,\n",
        "            temperature=0.8\n",
        "        )\n",
        "        return resp.choices[0].message.content.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating description: {e}\")\n",
        "        return f\"{energy_desc} {genre} vibes with {tempo_desc} rhythm\"\n",
        "\n",
        "\n",
        "def get_genre_folders(bucket, base_folder):\n",
        "    \"\"\"\n",
        "    Return a list of the immediate sub-directories under `base_folder/` in GCS.\n",
        "    e.g. ['ambient','blues','classical',‚Ä¶]\n",
        "    \"\"\"\n",
        "    iterator = bucket.list_blobs(prefix=f\"{base_folder}/\", delimiter=\"/\")\n",
        "    prefixes = set()\n",
        "\n",
        "    # You have to iterate over pages to get the .prefixes attribute\n",
        "    for page in iterator.pages:\n",
        "        prefixes.update(page.prefixes)\n",
        "\n",
        "    # each prefix looks like \"jamendo_by_genre/ambient/\"\n",
        "    genres = [p.rstrip(\"/\").split(\"/\")[-1] for p in prefixes]\n",
        "    return genres\n",
        "\n",
        "\n",
        "def sample_songs_from_genre(genre, n_samples=TRACKS_PER_GENRE):\n",
        "    \"\"\"Randomly sample n songs from a genre folder\"\"\"\n",
        "    blobs = list(bucket.list_blobs(prefix=f\"{BASE_FOLDER}/{genre}/\"))\n",
        "    mp3_blobs = [blob for blob in blobs if blob.name.endswith('.mp3')]\n",
        "\n",
        "    if len(mp3_blobs) < n_samples:\n",
        "        print(f\"Warning: Only {len(mp3_blobs)} songs available in {genre}, sampling all\")\n",
        "        return mp3_blobs\n",
        "\n",
        "    return random.sample(mp3_blobs, n_samples)\n",
        "\n",
        "def save_data_to_gcs(data, bucket, filename):\n",
        "    \"\"\"Save processed data to Google Cloud Storage\"\"\"\n",
        "    try:\n",
        "        # Convert data to JSON string\n",
        "        json_data = json.dumps(data, indent=2)\n",
        "\n",
        "        # Create blob and upload\n",
        "        blob = bucket.blob(filename)\n",
        "        blob.upload_from_string(json_data, content_type='application/json')\n",
        "\n",
        "        print(f\"üíæ Saved {filename} to GCS bucket: gs://{bucket.name}/{filename}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error saving to GCS: {e}\")\n",
        "        return False\n",
        "\n",
        "def load_data_from_gcs(bucket, filename):\n",
        "    \"\"\"Load processed data from Google Cloud Storage\"\"\"\n",
        "    try:\n",
        "        blob = bucket.blob(filename)\n",
        "        if blob.exists():\n",
        "            json_data = blob.download_as_text()\n",
        "            data = json.loads(json_data)\n",
        "            print(f\"üì• Loaded {filename} from GCS bucket ({len(data)} samples)\")\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è File {filename} not found in GCS bucket\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading from GCS: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_audio_file(blob, genre, feature_extractor, dcae_model):\n",
        "    \"\"\"Process a single audio file through the complete pipeline\"\"\"\n",
        "    try:\n",
        "        # Download file to temporary location\n",
        "        with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_file:\n",
        "            blob.download_to_filename(temp_file.name)\n",
        "            temp_path = temp_file.name\n",
        "\n",
        "        # Extract audio features\n",
        "        features = feature_extractor.extract_features(temp_path)\n",
        "        if features is None:\n",
        "            os.unlink(temp_path)\n",
        "            return None\n",
        "\n",
        "        # Generate vibe description\n",
        "        vibe_description = generate_vibe_description(features, genre, blob.name)\n",
        "\n",
        "        # Encode description using sentence transformer\n",
        "        prompt_embed = sentence_model.encode(vibe_description)\n",
        "\n",
        "        # Prepare mel spectrogram for DCAE\n",
        "        mel_spec = np.array(features['mel_spectrogram'])\n",
        "        mel_tensor = torch.FloatTensor(mel_spec).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
        "\n",
        "        # Get latent representation from DCAE\n",
        "        with torch.no_grad():\n",
        "            if torch.cuda.is_available():\n",
        "                mel_tensor = mel_tensor.cuda()\n",
        "                dcae_model = dcae_model.cuda()\n",
        "            latent = dcae_model.encode(mel_tensor).cpu().numpy().squeeze()\n",
        "\n",
        "        # Clean up temp file\n",
        "        os.unlink(temp_path)\n",
        "\n",
        "        return {\n",
        "            'filename': blob.name,\n",
        "            'genre': genre,\n",
        "            'vibe_description': vibe_description,\n",
        "            'prompt_embed': prompt_embed.tolist(),\n",
        "            'latent': latent.tolist(),\n",
        "            'audio_features': {k: v for k, v in features.items() if k != 'mel_spectrogram'}\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {blob.name}: {e}\")\n",
        "        if 'temp_path' in locals():\n",
        "            try:\n",
        "                os.unlink(temp_path)\n",
        "            except:\n",
        "                pass\n",
        "        return None\n",
        "\n",
        "class MusicVibeDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for music vibe data\"\"\"\n",
        "\n",
        "    def __init__(self, data_list):\n",
        "        self.data = data_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        return {\n",
        "            'prompt_embed': torch.FloatTensor(item['prompt_embed']),\n",
        "            'latent': torch.FloatTensor(item['latent']),\n",
        "            'genre': item['genre'],\n",
        "            'vibe_description': item['vibe_description'],\n",
        "            'filename': item['filename']\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main pipeline execution\"\"\"\n",
        "    print(\"üéµ Starting Music Vibe Processing Pipeline...\")\n",
        "\n",
        "    # Initialize models\n",
        "    print(\"Initializing models...\")\n",
        "    feature_extractor = AudioFeatureExtractor()\n",
        "    dcae_model = SimpleDCAE()\n",
        "    dcae_model.eval()\n",
        "\n",
        "    # Get all genres\n",
        "    print(\"Discovering genres...\")\n",
        "    genres = get_genre_folders(bucket, BASE_FOLDER)\n",
        "    print(f\"Found {genres}:\")\n",
        "    for i, genre in enumerate(genres):\n",
        "        print(f\"  {i+1}. {genre}\")\n",
        "\n",
        "    if not genres:\n",
        "        print(\"‚ùå No valid genre folders found!\")\n",
        "        print(\"Please check your bucket structure. Expected: jamendo_by_genre/genre_name/*.mp3\")\n",
        "\n",
        "        # Debug: Show what's actually in the bucket\n",
        "        print(\"\\nüîç Debugging - showing bucket contents:\")\n",
        "        all_blobs = list(bucket.list_blobs(prefix=f\"{BASE_FOLDER}/\", max_results=20))\n",
        "        for blob in all_blobs:\n",
        "            print(f\"  {blob.name}\")\n",
        "\n",
        "        return None, []\n",
        "\n",
        "    # Process all songs\n",
        "    all_data = []\n",
        "\n",
        "    for genre in tqdm(genres, desc=\"Processing genres\"):\n",
        "        print(f\"\\nüé∂ Processing genre: {genre}\")\n",
        "\n",
        "        # Sample songs from this genre\n",
        "        sampled_blobs = sample_songs_from_genre(genre)\n",
        "        if not sampled_blobs:\n",
        "            print(f\"‚ö†Ô∏è No MP3 files found in genre: {genre}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {len(sampled_blobs)} songs from {genre}\")\n",
        "\n",
        "        # Process each song\n",
        "        genre_data = []\n",
        "        for blob in tqdm(sampled_blobs, desc=f\"Processing {genre} songs\", leave=False):\n",
        "            result = process_audio_file(blob, genre, feature_extractor, dcae_model)\n",
        "            if result:\n",
        "                genre_data.append(result)\n",
        "\n",
        "        all_data.extend(genre_data)\n",
        "        print(f\"Successfully processed {len(genre_data)} songs from {genre}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Pipeline complete! Processed {len(all_data)} songs total.\")\n",
        "\n",
        "    if not all_data:\n",
        "        print(\"‚ùå No songs were processed successfully!\")\n",
        "        print(\"This might be due to:\")\n",
        "        print(\"1. Incorrect bucket structure\")\n",
        "        print(\"2. No MP3 files in the expected locations\")\n",
        "        print(\"3. Permission issues\")\n",
        "        return None, []\n",
        "\n",
        "    # Save processed data to GCS bucket\n",
        "    save_data_to_gcs(all_data, bucket, 'processed_music_vibe_data.json')\n",
        "\n",
        "    # Also save locally for immediate use\n",
        "    with open('music_vibe_data.json', 'w') as f:\n",
        "        json.dump(all_data, f, indent=2)\n",
        "    print(\"üíæ Saved raw data locally and to GCS bucket\")\n",
        "\n",
        "    # Create PyTorch dataset\n",
        "    dataset = MusicVibeDataset(all_data)\n",
        "    print(f\"üìä Created PyTorch dataset with {len(dataset)} samples\")\n",
        "\n",
        "    # Create DataLoader for training\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    print(\"üîÑ DataLoader ready for training\")\n",
        "\n",
        "    # Print sample data\n",
        "    if len(all_data) > 0:\n",
        "        sample = all_data[0]\n",
        "        print(f\"\\nüìã Sample data structure:\")\n",
        "        print(f\"Genre: {sample['genre']}\")\n",
        "        print(f\"Vibe: {sample['vibe_description']}\")\n",
        "        print(f\"Prompt embedding shape: {np.array(sample['prompt_embed']).shape}\")\n",
        "        print(f\"Latent representation shape: {np.array(sample['latent']).shape}\")\n",
        "\n",
        "    return dataset, all_data\n",
        "\n",
        "# Usage example\n",
        "if __name__ == \"__main__\":\n",
        "    # Set your OpenAI API key before running\n",
        "    # openai.api_key = \"your-key-here\"\n",
        "\n",
        "    # Example of loading previously saved data\n",
        "    # saved_data = load_data_from_gcs(bucket, 'processed_music_vibe_data.json')\n",
        "    # if saved_data:\n",
        "    #     print(f\"Found existing processed data with {len(saved_data)} samples\")\n",
        "\n",
        "    dataset, processed_data = main()\n",
        "\n",
        "    # Example of using the dataset\n",
        "    print(f\"\\nüéØ Dataset ready with {len(dataset)} samples\")\n",
        "    print(\"You can now use this dataset for training your models!\")\n",
        "\n",
        "    # Quick test of DataLoader\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "    batch = next(iter(dataloader))\n",
        "    print(f\"Batch keys: {batch.keys()}\")\n",
        "    print(f\"Batch prompt_embed shape: {batch['prompt_embed'].shape}\")\n",
        "    print(f\"Batch latent shape: {batch['latent'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cjFlsAxLJtHI",
        "outputId": "96e1de93-ed8b-42d2-b61b-92b030ec70f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                            filename genre  \\\n",
            "0       jamendo_by_genre/jazz/18_AdHoc_No_border.mp3  jazz   \n",
            "1  jamendo_by_genre/jazz/07_Azinity_Music_Banoffe...  jazz   \n",
            "2  jamendo_by_genre/jazz/12_The_Good_Lawdz_Eviden...  jazz   \n",
            "3  jamendo_by_genre/jazz/00_Omotesound_Tokyo_Nigh...  jazz   \n",
            "4  jamendo_by_genre/jazz/02_Olepash_Dream_in_a_Su...  jazz   \n",
            "5  jamendo_by_genre/jazz/03_pinegroove_Jazz_Cockt...  jazz   \n",
            "6  jamendo_by_genre/jazz/08_Mark_Dorricott_Softly...  jazz   \n",
            "7      jamendo_by_genre/jazz/17_Manuzik_JazzProg.mp3  jazz   \n",
            "8  jamendo_by_genre/jazz/11_pinegroove_Sexy_Smoot...  jazz   \n",
            "9  jamendo_by_genre/jazz/01_Omotesound_Tokyo_Lumi...  jazz   \n",
            "\n",
            "                                    vibe_description  \n",
            "0  Twilight jazz club, dim lights, smoky ambiance...  \n",
            "1  Swaying under dimly lit jazz club lights, a co...  \n",
            "2  Moonlit stroll through a vibrant city, gentle ...  \n",
            "3  Embark on a bustling midnight stroll through a...  \n",
            "4  Sipping coffee in a cozy, dimly lit jazz club,...  \n",
            "5  An energetic sunset in the city, where saxopho...  \n",
            "6  A bustling jazz club at midnight, where the wa...  \n",
            "7  Indulge in a lively jazz club scene with sultr...  \n",
            "8  Sultry saxophone whispers secrets in a dimly l...  \n",
            "9  Dive into a smoky jazz club, where the music d...  \n",
            "        tempo  rms_mean  spectral_centroid_mean\n",
            "count  287.00    287.00                  287.00\n",
            "mean   116.54      0.14                 1819.90\n",
            "std     29.34      0.08                  729.89\n",
            "min     45.33      0.02                  176.47\n",
            "25%     95.70      0.08                 1283.56\n",
            "50%    117.45      0.13                 1826.03\n",
            "75%    129.20      0.19                 2302.31\n",
            "max    258.40      0.45                 4240.79\n",
            "genre\n",
            "jazz          20\n",
            "classical     20\n",
            "relaxation    20\n",
            "funk          20\n",
            "electronic    20\n",
            "hiphop        20\n",
            "metal         20\n",
            "reggae        20\n",
            "blues         20\n",
            "rock          20\n",
            "soundtrack    20\n",
            "world         20\n",
            "latin         20\n",
            "pop           17\n",
            "ambient       10\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# 1) Load into a DataFrame\n",
        "with open('music_vibe_data.json','r') as f:\n",
        "    data = json.load(f)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 2) Peek at the first few rows: filename, genre, vibe\n",
        "print(df[['filename','genre','vibe_description']].head(10))\n",
        "\n",
        "# 3) Flatten out your numeric audio features so you can summarize them:\n",
        "features = pd.json_normalize(df['audio_features'])\n",
        "print(features[['tempo','rms_mean','spectral_centroid_mean']].describe().round(2))\n",
        "\n",
        "# 4) Check that every genre has exactly 20 songs:\n",
        "print(df['genre'].value_counts())\n",
        "\n",
        "# 5) Visualize the tempo distribution:\n",
        "plt.figure()\n",
        "features['tempo'].hist(bins=30)\n",
        "plt.xlabel('Tempo (BPM)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Tempo Distribution Across All Songs')\n",
        "plt.show()\n",
        "\n",
        "# 6) Quick 2-D embedding of your latent space to see clustering by genre\n",
        "#    (you‚Äôll need sklearn installed; this is just for exploration)\n",
        "latent_df = pd.DataFrame(df['latent'].tolist())\n",
        "pca = PCA(n_components=50).fit_transform(latent_df)      # preshrink\n",
        "tsne = TSNE(n_components=2, init='pca', learning_rate='auto').fit_transform(pca)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(tsne[:,0], tsne[:,1], c=pd.factorize(df['genre'])[0], s=8)\n",
        "plt.title('t-SNE of DCAE Latents (colored by genre)')\n",
        "plt.xticks([]); plt.yticks([])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85Wb1uSuM4XQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3krT6O5_15cZ"
      },
      "source": [
        "# Data Processing for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9v45IIxXwMR"
      },
      "source": [
        "If you haven't already, set up ACE-Step. Paste the following in the terminal:\n",
        "git clone https://github.com/ace-step/ACE-Step.git\n",
        "\n",
        "cd ACE-Step\n",
        "\n",
        "python -m venv venv\n",
        "\n",
        "source venv/bin/activate\n",
        "\n",
        "pip install -e .\n",
        "\n",
        "You can run it here: acestep --port 7865\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1fD5Yg5Yl9p"
      },
      "source": [
        "Now that we have ACE-Step set up, we can now start to format the data appropriately.\n",
        "\n",
        "The script below takes the files currently in GCP and uploads them to the disk. It also filters and samples, so we have a small amount of data to train the model on.\n",
        "\n",
        "we'll be randomly selecting 5 seconds from each song to train the model on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "08cd55b7c44c4109ad39f3e0b75e4e15",
            "c92605dfe99d45aba79963c52a5ad36f",
            "69a052056d9f4f9ebbf1737cbd20b83d",
            "7d3c6f55ced347b6b87c2949994f1cdf",
            "f303235752c24851bc34874c2ef86512",
            "1003a56e2f7c44848ee5a8970261c108",
            "92ff55892c274304844c50a757b91902",
            "b130c3ca95014357847ba54d4e5ecf87",
            "1e060a11a2d9471aa8d3c4b22e17405e",
            "1d39be46280643a2a92115d3d07d18cf",
            "23461e7b5f384e5fa36dd2b9cc0562d7"
          ]
        },
        "id": "5AJ5A7wk18M_",
        "outputId": "f6ae915c-057e-4128-f64b-bdbdc53e1fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 287 items from GCS JSON\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading audio (parallel): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 17262.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Downloaded 200 audio files.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Cropping audio to 5.0s (parallel): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:16<00:00, 12.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Cropping complete.\n",
            "After cropping: Min: 5.44, Max: 5.44, Mean: 5.44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filtering + formatting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 199/200 [00:02<00:00, 94.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample reformatted entry:\n",
            "{\n",
            "  \"filename\": \"/content/jamendo_data/jamendo_by_genre/jazz/18_AdHoc_No_border.mp3\",\n",
            "  \"keys\": \"18_AdHoc_No_border\",\n",
            "  \"tags\": [\n",
            "    \"jazz\"\n",
            "  ],\n",
            "  \"norm_lyrics\": \"\",\n",
            "  \"recaption\": {},\n",
            "  \"speaker_emb_path\": null\n",
            "}\n",
            "Final dataset size: 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08cd55b7c44c4109ad39f3e0b75e4e15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Saved HuggingFace dataset to /content/my_hf_ace_dataset\n"
          ]
        }
      ],
      "source": [
        "# --- Install dependencies (run first in Colab) ---\n",
        "# !pip install gcsfs datasets pandas tqdm torchaudio\n",
        "\n",
        "import os\n",
        "import json\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "import torchaudio\n",
        "import random\n",
        "import concurrent.futures\n",
        "\n",
        "BUCKET_NAME = \"uchicago-bayesian-bayes-beats\"\n",
        "GCS_JSON_PATH = \"processed_music_vibe_data.json\"\n",
        "LOCAL_DATA_DIR = \"/content/jamendo_data\"\n",
        "LOCAL_JSON_PATH = \"/content/music_vibe_data.json\"\n",
        "HF_DATASET_PATH = \"/content/my_hf_ace_dataset\"\n",
        "\n",
        "MAX_DURATION_SECONDS = 5.0\n",
        "SAMPLE_SIZE = 200\n",
        "SAMPLE_RATE = 48000\n",
        "\n",
        "# --- Colab GCS Auth ---\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# --- Load JSON Metadata ---\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "with fs.open(f\"{BUCKET_NAME}/{GCS_JSON_PATH}\", 'rb') as f:\n",
        "    data = json.load(f)\n",
        "print(f\"Loaded {len(data)} items from GCS JSON\")\n",
        "\n",
        "if SAMPLE_SIZE is not None:\n",
        "    data = data[:SAMPLE_SIZE]\n",
        "\n",
        "# --- Download audio files using concurrent.futures ---\n",
        "def download_one_audio(gcs_path, local_data_dir=LOCAL_DATA_DIR):\n",
        "    # Remove bucket prefix if present\n",
        "    local_gcs_path = gcs_path\n",
        "    if gcs_path.startswith(BUCKET_NAME + \"/\"):\n",
        "        local_gcs_path = gcs_path[len(BUCKET_NAME) + 1:]\n",
        "    local_path = os.path.join(local_data_dir, local_gcs_path)\n",
        "    local_dir = os.path.dirname(local_path)\n",
        "    os.makedirs(local_dir, exist_ok=True)\n",
        "    if not os.path.exists(local_path):\n",
        "        try:\n",
        "            with fs.open(f\"{BUCKET_NAME}/{local_gcs_path}\", 'rb') as fsrc:\n",
        "                with open(local_path, 'wb') as fdst:\n",
        "                    fdst.write(fsrc.read())\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to download {gcs_path}: {e}\")\n",
        "            return None\n",
        "    return local_path\n",
        "\n",
        "def download_audio_files_parallel(data, audio_col='filename', local_data_dir=LOCAL_DATA_DIR, max_workers=16):\n",
        "    gcs_audio_paths = list(set(d[audio_col] for d in data))\n",
        "    local_audio_paths = []\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(tqdm(\n",
        "            executor.map(lambda path: download_one_audio(path, local_data_dir), gcs_audio_paths),\n",
        "            total=len(gcs_audio_paths), desc=\"Downloading audio (parallel)\"\n",
        "        ))\n",
        "    local_audio_paths = [r for r in results if r is not None]\n",
        "    print(f\"‚úÖ Downloaded {len(local_audio_paths)} audio files.\")\n",
        "    return local_audio_paths\n",
        "\n",
        "local_audio_paths = download_audio_files_parallel(data, max_workers=16)\n",
        "\n",
        "# --- Crop audio to MAX_DURATION_SECONDS in parallel ---\n",
        "def crop_one_audio(path, max_seconds=5, sample_rate=48000):\n",
        "    try:\n",
        "        audio, sr = torchaudio.load(path)\n",
        "        max_samples = int(sample_rate * max_seconds)\n",
        "        if audio.shape[-1] > max_samples:\n",
        "            start = random.randint(0, audio.shape[-1] - max_samples)\n",
        "            audio = audio[:, start : start + max_samples]\n",
        "            torchaudio.save(path, audio, sr)\n",
        "        elif audio.shape[-1] < max_samples:\n",
        "            audio = torch.nn.functional.pad(audio, (0, max_samples - audio.shape[-1]), 'constant', 0)\n",
        "            torchaudio.save(path, audio, sr)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {path}: {e}\")\n",
        "        return False\n",
        "\n",
        "def crop_one_audio_star(args):\n",
        "    return crop_one_audio(*args)\n",
        "\n",
        "def crop_audio_to_duration_parallel(audio_paths, max_seconds=5, sample_rate=48000, max_workers=16):\n",
        "    args = [(path, max_seconds, sample_rate) for path in audio_paths]\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        list(tqdm(\n",
        "            executor.map(crop_one_audio_star, args),\n",
        "            total=len(audio_paths), desc=f\"Cropping audio to {max_seconds}s (parallel)\"\n",
        "        ))\n",
        "    print(\"‚úÖ Cropping complete.\")\n",
        "\n",
        "\n",
        "crop_audio_to_duration_parallel(local_audio_paths, MAX_DURATION_SECONDS, SAMPLE_RATE, max_workers=16)\n",
        "\n",
        "# --- Check durations after cropping (debug) ---\n",
        "durations = []\n",
        "for path in local_audio_paths:\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        duration = info.num_frames / info.sample_rate\n",
        "        durations.append(duration)\n",
        "        if duration > MAX_DURATION_SECONDS + 0.5:\n",
        "            print(f\"‚ùó Warning: {path} still longer than {MAX_DURATION_SECONDS}s ({duration}s)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading duration for {path}: {e}\")\n",
        "\n",
        "if durations:\n",
        "    print(f\"After cropping: Min: {min(durations):.2f}, Max: {max(durations):.2f}, Mean: {sum(durations)/len(durations):.2f}\")\n",
        "else:\n",
        "    print(\"‚ùå No durations found after cropping!\")\n",
        "\n",
        "# --- Filter + Reformat JSON to ACE format ---\n",
        "def get_duration(path):\n",
        "    try:\n",
        "        info = torchaudio.info(path)\n",
        "        return info.num_frames / info.sample_rate\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "recs = []\n",
        "kept = 0\n",
        "for i, row in enumerate(tqdm(data, desc=\"Filtering + formatting\")):\n",
        "    gcs_audio_path = row[\"filename\"]\n",
        "    if gcs_audio_path.startswith(BUCKET_NAME + \"/\"):\n",
        "        gcs_audio_path = gcs_audio_path[len(BUCKET_NAME)+1:]\n",
        "    local_audio_path = os.path.join(LOCAL_DATA_DIR, gcs_audio_path)\n",
        "    duration = get_duration(local_audio_path)\n",
        "    if duration is None:\n",
        "        print(f\"‚ùå Skipping {local_audio_path} (duration=None)\")\n",
        "        continue\n",
        "    if (MAX_DURATION_SECONDS is not None) and (duration > MAX_DURATION_SECONDS + 0.5):\n",
        "        print(f\"‚ùå Skipping {local_audio_path} (too long: {duration}s)\")\n",
        "        continue\n",
        "    recs.append({\n",
        "        \"filename\": local_audio_path,\n",
        "        \"keys\": os.path.splitext(os.path.basename(gcs_audio_path))[0],\n",
        "        \"tags\": [row[\"genre\"]],\n",
        "        \"norm_lyrics\": \"\",\n",
        "        \"recaption\": {},\n",
        "        \"speaker_emb_path\": None\n",
        "    })\n",
        "    kept += 1\n",
        "    if SAMPLE_SIZE and kept >= SAMPLE_SIZE:\n",
        "        break\n",
        "\n",
        "if recs:\n",
        "    print(f\"Sample reformatted entry:\\n{json.dumps(recs[0], indent=2)}\")\n",
        "else:\n",
        "    print(\"‚ùå No valid records!\")\n",
        "print(f\"Final dataset size: {len(recs)}\")\n",
        "\n",
        "# --- Save as Hugging Face dataset ---\n",
        "if recs:\n",
        "    df = pd.DataFrame(recs)\n",
        "    hf_ds = Dataset.from_pandas(df)\n",
        "    hf_ds.save_to_disk(HF_DATASET_PATH)\n",
        "    print(\"‚úÖ Saved HuggingFace dataset to\", HF_DATASET_PATH)\n",
        "else:\n",
        "    print(\"‚ùå No records to save!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LCo3dDE5EKM",
        "outputId": "3720618f-a9b0-423b-a0a4-d29bae83d2fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.0)\n"
          ]
        }
      ],
      "source": [
        "# we need to run more code to extact the _prompt.txt and _lyrics.txt files for each MP3 to be able to run the LORA training script.\n",
        "#!pip install joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6otiXPX5OZq",
        "outputId": "19107303-da69-4617-b8ba-3bc95eb56758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating prompt and lyrics files for 200 mp3s...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:00<00:00, 567.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All prompt and lyrics files created!\n",
            "Example output files:\n",
            "Prompt file: /content/jamendo_data/jamendo_by_genre/relaxation/12_Kenny_Leonore_Midnight_Mystique_of_Mont_Choisy_prompt.txt\n",
            "Lyrics file: /content/jamendo_data/jamendo_by_genre/relaxation/12_Kenny_Leonore_Midnight_Mystique_of_Mont_Choisy_lyrics.txt\n",
            "Prompt contents: relaxation\n",
            "Lyrics contents: ''\n",
            "Prompt file: /content/jamendo_data/jamendo_by_genre/relaxation/13_PeryCreep_Lofi_for_relax_prompt.txt\n",
            "Lyrics file: /content/jamendo_data/jamendo_by_genre/relaxation/13_PeryCreep_Lofi_for_relax_lyrics.txt\n",
            "Prompt contents: relaxation\n",
            "Lyrics contents: ''\n",
            "Prompt file: /content/jamendo_data/jamendo_by_genre/relaxation/14_Kenny_Leonore_Chuchotements_parmi_les_Canna_Indica_prompt.txt\n",
            "Lyrics file: /content/jamendo_data/jamendo_by_genre/relaxation/14_Kenny_Leonore_Chuchotements_parmi_les_Canna_Indica_lyrics.txt\n",
            "Prompt contents: relaxation\n",
            "Lyrics contents: ''\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load your metadata\n",
        "with open('/content/music_vibe_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "# Build fast lookup: <basename> -> genre/tag\n",
        "genre_by_basename = {os.path.basename(d['filename']): d['genre'] for d in data}\n",
        "\n",
        "def create_prompt_and_lyrics(mp3_path, genre_lookup):\n",
        "    stem = mp3_path[:-4]\n",
        "    prompt_path = stem + '_prompt.txt'\n",
        "    lyrics_path = stem + '_lyrics.txt'\n",
        "    basename = os.path.basename(mp3_path)\n",
        "    genre = genre_lookup.get(basename, \"music\")\n",
        "    # Write prompt (genre as string)\n",
        "    with open(prompt_path, \"w\") as pf:\n",
        "        pf.write(genre)\n",
        "    # Write empty lyrics\n",
        "    with open(lyrics_path, \"w\") as lf:\n",
        "        lf.write(\"\")\n",
        "    return (prompt_path, lyrics_path)\n",
        "\n",
        "# Find all mp3s under jamendo_data/jamendo_by_genre/*\n",
        "mp3_files = []\n",
        "for root, dirs, files in os.walk('/content/jamendo_data/jamendo_by_genre'):\n",
        "    for file in files:\n",
        "        if file.endswith('.mp3'):\n",
        "            mp3_files.append(os.path.join(root, file))\n",
        "\n",
        "# Parallel file creation (uses all available CPUs)\n",
        "print(f\"Creating prompt and lyrics files for {len(mp3_files)} mp3s...\")\n",
        "results = Parallel(n_jobs=-1)(\n",
        "    delayed(create_prompt_and_lyrics)(mp3_path, genre_by_basename)\n",
        "    for mp3_path in tqdm(mp3_files)\n",
        ")\n",
        "print(\"‚úÖ All prompt and lyrics files created!\")\n",
        "\n",
        "# Sanity check on first few\n",
        "print(\"Example output files:\")\n",
        "for i in range(min(3, len(results))):\n",
        "    print(\"Prompt file:\", results[i][0])\n",
        "    print(\"Lyrics file:\", results[i][1])\n",
        "    with open(results[i][0]) as pf:\n",
        "        print(\"Prompt contents:\", pf.read())\n",
        "    with open(results[i][1]) as lf:\n",
        "        print(\"Lyrics contents:\", repr(lf.read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H06s_BeH5xur"
      },
      "source": [
        "Now we have everything formatted the way ACE step expects for training.\n",
        "\n",
        "Since the MP3s are in subfolders, change line 9 of ACE-Step/convert2hf_dataset.py to \"for song_path in data_path.glob(\"**/*.mp3\"):\" --> lets us look at subdirectories.\n",
        "\n",
        "Run the cell below for further conversion/ingestion into the data pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkSKmAIO57iA",
        "outputId": "5e839e08-6748-4f9a-fd07-1a247c723bff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ACE-Step\n",
            "Saving the dataset (1/1 shards): 100% 200/200 [00:00<00:00, 51861.56 examples/s]\n"
          ]
        }
      ],
      "source": [
        "%cd /content/ACE-Step\n",
        "!python convert2hf_dataset.py --data_dir \"/content/jamendo_data/jamendo_by_genre\" --repeat_count 1 --output_name \"/content/my_hf_ace_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_FdAnK0TVHS",
        "outputId": "6d266848-cb1a-4684-8b2b-4a89dd836cf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min duration: 5.442176870748299\n",
            "Max duration: 5.442176870748299\n",
            "Avg duration: 5.4421768707483045\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "\n",
        "durations = []\n",
        "for mp3_path in mp3_files:\n",
        "    try:\n",
        "        info = torchaudio.info(mp3_path)\n",
        "        durations.append(info.num_frames / info.sample_rate)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to check {mp3_path}: {e}\")\n",
        "\n",
        "print(\"Min duration:\", min(durations))\n",
        "print(\"Max duration:\", max(durations))\n",
        "print(\"Avg duration:\", sum(durations) / len(durations))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFW9TgKW6HES",
        "outputId": "e2ba366a-b72d-4915-a9de-dcef8f91991e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['keys', 'filename', 'tags', 'speaker_emb_path', 'norm_lyrics', 'recaption'],\n",
            "    num_rows: 200\n",
            "})\n",
            "{'keys': '12_Kenny_Leonore_Midnight_Mystique_of_Mont_Choisy', 'filename': '/content/jamendo_data/jamendo_by_genre/relaxation/12_Kenny_Leonore_Midnight_Mystique_of_Mont_Choisy.mp3', 'tags': ['relaxation'], 'speaker_emb_path': '', 'norm_lyrics': '', 'recaption': {}}\n"
          ]
        }
      ],
      "source": [
        "# sanity check here:\n",
        "from datasets import load_from_disk\n",
        "ds = load_from_disk(\"/content/my_hf_ace_dataset\")\n",
        "print(ds)\n",
        "print(ds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya1BdjQ7PLXR"
      },
      "source": [
        "#TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5pSlIWI7ADb"
      },
      "source": [
        "Now that we have our data formatted, we're ready to train. The problem with this is our model is huge, and we're running out of VRAM. Before we atempt to fine-tune, we need to do a couple of things.\n",
        "\n",
        "Before running the cells below, make sure to restart the runtime if on Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8kCnPw8BufW",
        "outputId": "ed0af4c8-844d-470e-dfcb-95f7865e3b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun May 25 20:33:47 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             51W /  400W |   14291MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "MAX_LYRIC_LENGTH = 1024      # Or what your model expects (adjust if needed)\n",
        "MAX_AUDIO_LENGTH = 24 * 10 * 48000  # 240 seconds at 48kHz; adjust to your needs\n",
        "MAX_CHUNK_MASK_LENGTH = 512  # Example for chunk_masks etc., set as appropriate\n",
        "MAX_CLAP_ATTENTION_LENGTH = 512  # Example for clap_attention_masks, adjust if needed\n",
        "MAX_CLAP_CONDITION_LENGTH = 1024 # Adjust if needed, for time dim in clap_conditions\n",
        "\n",
        "\n",
        "import torch\n",
        "!nvidia-smi # make sure your cache is super clean!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FskyR-ihB2Dt",
        "outputId": "e068a4a5-dcf2-4cee-88d5-adcdba6682e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n",
            "Collecting nnaudio\n",
            "  Downloading nnAudio-0.3.3-py3-none-any.whl.metadata (771 bytes)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nnaudio) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.11/dist-packages (from nnaudio) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from nnaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->nnaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->nnaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->nnaudio) (3.0.2)\n",
            "Downloading nnAudio-0.3.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nnaudio\n",
            "Successfully installed nnaudio-0.3.3\n"
          ]
        }
      ],
      "source": [
        "# !pip install bitsandbytes\n",
        "# !pip install nnaudio\n",
        "import bitsandbytes as bnb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MJFBxuxGUXW"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxj_wQOzXMHe"
      },
      "outputs": [],
      "source": [
        "#!fuser -k /dev/nvidia*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftpZzimS8OJo",
        "outputId": "e88538e8-320f-4887-966b-30edab3d9ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00ab60>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00ab60>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b0093f0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b0093f0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00bb80>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00bb80>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b2e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b2e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009f60>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009f60>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009690>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009690>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b760>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b760>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b141570>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b141570>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b141630>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b141630>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1414b0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1414b0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b140d00>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b140d00>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b141690>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b141690>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009a20>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009a20>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00a200>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00a200>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b008400>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b008400>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009f90>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009f90>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b0093c0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b0093c0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b008700>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b008700>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009d20>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009d20>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00aec0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00aec0>\n",
            "Epoch 75:  90% 180/200 [02:07<00:14,  1.41it/s, v_num=tune]\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009a50>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009a50>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00a4a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00a4a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1411b0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1411b0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1414b0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1414b0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1415a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1415a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b141d20>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b141d20>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b220>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b220>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b2e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b2e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00a3e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00a3e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00bc10>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00bc10>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00ab60>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00ab60>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00bdc0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00bdc0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b008f10>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b008f10>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b009d50>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b009d50>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b0083d0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b0083d0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b100>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b100>\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00a620>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00a620>\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b970>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b970>\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00b100>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00b100>\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b00a920>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b00a920>\n",
            "Epoch 75: 100% 200/200 [02:21<00:00,  1.42it/s, v_num=tune]\u001b[32m2025-05-26 02:09:53.070\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.text2music_dataset\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m205\u001b[0m - \u001b[1mDataset size: 200 total 200 samples\u001b[0m\n",
            "Epoch 76:   0% 0/200 [00:00<?, ?it/s, v_num=tune]\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b141750>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b141750>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecd450>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecd450>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecead0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecead0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199270>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199270>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19b070>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19b070>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199000>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199000>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199fc0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199fc0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1991b0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1991b0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199c60>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199c60>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198490>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198490>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199b40>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199b40>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199120>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199120>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19b460>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19b460>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1999c0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1999c0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecddb0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecddb0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecdae0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecdae0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198af0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198af0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199030>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199030>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a9e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a9e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198f40>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198f40>\n",
            "Epoch 76:  10% 20/200 [00:14<02:13,  1.35it/s, v_num=tune]\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a710>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a710>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1987c0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1987c0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a1a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a1a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198490>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198490>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198370>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198370>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1997b0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1997b0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a080>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a080>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19ac20>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19ac20>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecfd30>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecfd30>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecd1e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecd1e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199cf0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199cf0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1980a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1980a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1995d0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1995d0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a5f0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a5f0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199cc0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199cc0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199960>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199960>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19b3a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19b3a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19ae60>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19ae60>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19ba90>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19ba90>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1997b0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1997b0>\n",
            "Epoch 76:  20% 40/200 [00:28<01:55,  1.38it/s, v_num=tune]\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199db0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199db0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198f70>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198f70>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecf9d0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecf9d0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecfbb0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecfbb0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199ff0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199ff0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198370>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198370>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198af0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198af0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a5f0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a5f0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a350>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a350>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1998d0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1998d0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198850>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198850>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19acb0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19acb0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a290>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a290>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199360>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199360>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a9e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a9e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1998a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1998a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecec80>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecec80>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecd540>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecd540>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7db7ecf340>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7db7ecf340>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19ae30>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19ae30>\n",
            "Epoch 76:  30% 60/200 [00:42<01:40,  1.40it/s, v_num=tune]\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a1d0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a1d0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199270>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199270>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1980a0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1980a0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1998d0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1998d0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b1982e0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b1982e0>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a560>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a560>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199f60>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199f60>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b198220>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b198220>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b19a020>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b19a020>\n",
            "\n",
            "[collate_fn] Processing key: keys | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: target_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for target_wavs --\n",
            "    [target_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in target_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for target_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: vocal_wavs | Batch len: 1\n",
            "  [collate_fn] -- AUDIO block for vocal_wavs --\n",
            "    [vocal_wavs][0] shape: torch.Size([2, 261225]), type: <class 'torch.Tensor'>, dim: 2\n",
            "  [collate_fn] Finished processing all items in vocal_wavs, stacking batch of 1\n",
            "  [collate_fn] Final stacked tensor for vocal_wavs: shape torch.Size([1, 11520000])\n",
            "\n",
            "[collate_fn] Processing key: wav_lengths | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: structured_tags | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: prompts | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: speaker_embs | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_token_ids | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: lyric_masks | Batch len: 1\n",
            "\n",
            "[collate_fn] Processing key: candidate_lyric_chunks | Batch len: 1\n",
            "[collate_fn] Finished batch collation.\n",
            "  output['keys']: list of length 1\n",
            "  output['target_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['vocal_wavs']: torch.Size([1, 11520000]) dtype=torch.float32\n",
            "  output['wav_lengths']: torch.Size([1]) dtype=torch.int64\n",
            "  output['structured_tags']: list of length 1\n",
            "  output['prompts']: list of length 1\n",
            "  output['speaker_embs']: torch.Size([1, 512]) dtype=torch.float32\n",
            "  output['lyric_token_ids']: torch.Size([1, 1024]) dtype=torch.int64\n",
            "  output['lyric_masks']: torch.Size([1, 1024]) dtype=torch.float32\n",
            "  output['candidate_lyric_chunks']: list of length 1\n",
            "  output['cfg_mask_text']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_speaker']: torch.Size([1]) dtype=torch.int64\n",
            "  output['cfg_mask_lyrics']: torch.Size([1]) dtype=torch.int64\n",
            "transformer_output.sample.requires_grad: True\n",
            "After transformer ‚Üí model_pred.requires_grad = True  grad_fn = <CloneBackward0 object at 0x7c7f6b199fc0>\n",
            "Final loss ‚Üí requires_grad = True  grad_fn = <MeanBackward0 object at 0x7c7f6b199fc0>\n",
            "\n",
            "Detected KeyboardInterrupt, attempting graceful shutdown ...\n",
            "Epoch 76:  30% 60/200 [01:02<02:24,  1.03s/it, v_num=tune]\n"
          ]
        }
      ],
      "source": [
        "!python /content/ACE-Step/trainer.py \\\n",
        "  --dataset_path \"/content/my_hf_ace_dataset\" \\\n",
        "  --exp_name \"ace_lora_finetune\" \\\n",
        "  --logger_dir \"/content/exps/logs/\" \\\n",
        "  --checkpoint_dir \"/content/exps/checkpoints/\" \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --num_workers 2 \\\n",
        "  --accumulate_grad_batches 8 \\\n",
        "  --devices 1 \\\n",
        "  --precision 16 \\\n",
        "  --max_steps 5000 \\\n",
        "  --every_plot_step 1000000 \\\n",
        "  --every_n_train_steps 500 \\\n",
        "  --lora_config_path \"/content/ACE-Step/config/zh_rap_lora_config.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "lS6XFRtcUPMM",
        "outputId": "c2c9b7ce-6366-4f60-e2a8-99cb6ce7b1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uploading /content/exps/logs/lightning_logs/2025-05-25_22-27-11ace_lora_finetune/checkpoints/epoch=0-step=1.ckpt to gs://uchicago-bayesian-bayes-beats/checkpoints/ace-lora-finetune/test_run/epoch=0-step=1.ckpt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-ef00e97826ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# raises error if upload failed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-ef00e97826ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Upload in parallel (adjust max_workers as desired; 4 is a safe default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_ckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mckpt_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "auth.authenticate_user()\n",
        "bucket_name = \"uchicago-bayesian-bayes-beats\"\n",
        "local_checkpoint_dir = \"/content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune/checkpoints\"\n",
        "gcs_checkpoint_dir = \"checkpoints/ace-lora-finetune/test_run\"\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "def upload_ckpt(local_file_path, rel_path):\n",
        "    blob = bucket.blob(os.path.join(gcs_checkpoint_dir, rel_path))\n",
        "    print(f\"Uploading {local_file_path} to gs://{bucket_name}/{gcs_checkpoint_dir}/{rel_path}\")\n",
        "    blob.upload_from_filename(local_file_path)\n",
        "    print(f\"Uploaded {local_file_path}\")\n",
        "\n",
        "# Find all .ckpt files\n",
        "ckpt_files = []\n",
        "for root, dirs, files in os.walk(local_checkpoint_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".ckpt\"):\n",
        "            local_file_path = os.path.join(root, file)\n",
        "            rel_path = os.path.relpath(local_file_path, local_checkpoint_dir)\n",
        "            ckpt_files.append((local_file_path, rel_path))\n",
        "\n",
        "# Upload in parallel (adjust max_workers as desired; 4 is a safe default)\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(upload_ckpt, local_file_path, rel_path) for local_file_path, rel_path in ckpt_files]\n",
        "    for future in as_completed(futures):\n",
        "        future.result()  # raises error if upload failed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sI5DkKgFeqS"
      },
      "source": [
        "DO THIS IN TERMINAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elMQOo5Wm5Gg"
      },
      "outputs": [],
      "source": [
        "acestep --checkpoint_path /content/exps/logs/lightning_logs/2025-05-25_22-27-11ace_lora_finetune/checkpoints/epoch=0-step=1.ckpt --port 7865 --device_id 0 --share true --bf16 true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBqiXsaZVwiY",
        "outputId": "3df18abc-7033-4d16-d6c3-98151e106fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting checkpoint upload to Google Cloud Storage...\n",
            "Local file: /content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune/checkpoints/epoch=59-step=1500.ckpt\n",
            "Destination bucket: uchicago-bayesian-bayes-beats\n",
            "------------------------------------------------------------\n",
            "Successfully connected to bucket: uchicago-bayesian-bayes-beats\n",
            "File size: 16429.38 MB\n",
            "Starting upload to gs://uchicago-bayesian-bayes-beats/checkpoints/ace_lora_finetune_20250526_021904_epoch59_step1500.ckpt\n",
            "This may take several minutes for large files...\n",
            "‚úÖ Successfully uploaded checkpoint!\n",
            "GCS Path: gs://uchicago-bayesian-bayes-beats/checkpoints/ace_lora_finetune_20250526_021904_epoch59_step1500.ckpt\n",
            "File URL: https://storage.googleapis.com/uchicago-bayesian-bayes-beats/checkpoints/ace_lora_finetune_20250526_021904_epoch59_step1500.ckpt\n",
            "\n",
            "üéâ Upload completed successfully!\n",
            "üöÄ Starting checkpoint upload to Google Cloud Storage...\n",
            "Local file: /content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune/checkpoints/epoch=59-step=1500.ckpt\n",
            "Destination bucket: uchicago-bayesian-bayes-beats\n",
            "------------------------------------------------------------\n",
            "Successfully connected to bucket: uchicago-bayesian-bayes-beats\n",
            "File size: 16429.38 MB\n",
            "Starting upload to gs://uchicago-bayesian-bayes-beats/checkpoints/ace_lora_finetune_20250526_022248_epoch59_step1500.ckpt\n",
            "This may take several minutes for large files...\n",
            "‚úÖ Successfully uploaded checkpoint!\n",
            "GCS Path: gs://uchicago-bayesian-bayes-beats/checkpoints/ace_lora_finetune_20250526_022248_epoch59_step1500.ckpt\n",
            "File URL: https://storage.googleapis.com/uchicago-bayesian-bayes-beats/checkpoints/ace_lora_finetune_20250526_022248_epoch59_step1500.ckpt\n",
            "\n",
            "üéâ Upload completed successfully!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Script to upload a model checkpoint to Google Cloud Storage bucket\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "BUCKET_NAME = \"uchicago-bayesian-bayes-beats\"\n",
        "LOCAL_CHECKPOINT_PATH = \"/content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune/checkpoints/epoch=59-step=1500.ckpt\"\n",
        "\n",
        "def upload_checkpoint_to_gcs():\n",
        "    \"\"\"Upload the checkpoint file to Google Cloud Storage\"\"\"\n",
        "\n",
        "    # Initialize the GCS client\n",
        "    try:\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(BUCKET_NAME)\n",
        "        print(f\"Successfully connected to bucket: {BUCKET_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to GCS: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Check if local file exists\n",
        "    if not os.path.exists(LOCAL_CHECKPOINT_PATH):\n",
        "        print(f\"Error: Local checkpoint file not found at {LOCAL_CHECKPOINT_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Get file size for progress tracking\n",
        "    file_size = os.path.getsize(LOCAL_CHECKPOINT_PATH)\n",
        "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
        "\n",
        "    # Create destination path in bucket\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    destination_path = f\"checkpoints/ace_lora_finetune_{timestamp}_epoch59_step1500.ckpt\"\n",
        "\n",
        "    try:\n",
        "        # Create blob and upload\n",
        "        blob = bucket.blob(destination_path)\n",
        "\n",
        "        print(f\"Starting upload to gs://{BUCKET_NAME}/{destination_path}\")\n",
        "        print(\"This may take several minutes for large files...\")\n",
        "\n",
        "        # Upload the file\n",
        "        blob.upload_from_filename(LOCAL_CHECKPOINT_PATH)\n",
        "\n",
        "        print(f\"‚úÖ Successfully uploaded checkpoint!\")\n",
        "        print(f\"GCS Path: gs://{BUCKET_NAME}/{destination_path}\")\n",
        "        print(f\"File URL: https://storage.googleapis.com/{BUCKET_NAME}/{destination_path}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during upload: {e}\")\n",
        "        return False\n",
        "\n",
        "def main():\n",
        "    print(\"üöÄ Starting checkpoint upload to Google Cloud Storage...\")\n",
        "    print(f\"Local file: {LOCAL_CHECKPOINT_PATH}\")\n",
        "    print(f\"Destination bucket: {BUCKET_NAME}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    success = upload_checkpoint_to_gcs()\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nüéâ Upload completed successfully!\")\n",
        "    else:\n",
        "        print(\"\\nüí• Upload failed. Please check the error messages above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0boSe3ZOaKZU"
      },
      "outputs": [],
      "source": [
        "acestep --checkpoint_path /content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune/checkpoints/epoch=59-step=1500.ckpt --port 7865 --device_id 0 --share true --bf16 true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9PAgmQJb701",
        "outputId": "95133b11-d209-4d1d-b7f0-a16731630d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting full experiment folder upload to Google Cloud Storage...\n",
            "üìÇ Local folder: /content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune\n",
            "ü™£ Destination bucket: uchicago-bayesian-bayes-beats\n",
            "------------------------------------------------------------\n",
            "‚úÖ Successfully connected to bucket: uchicago-bayesian-bayes-beats\n",
            "üìÅ Scanning folder: /content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune\n",
            "üìä Found 32 files to upload\n",
            "üì¶ Total size: 48.50 GB\n",
            "üöÄ Starting upload to gs://uchicago-bayesian-bayes-beats/experiments/ace_lora_finetune_20250526_023307/\n",
            "============================================================\n",
            "üì§ Uploading: events.out.tfevents.1748213977.eb8dd917cc36.101646.0 (0.0 MB)\n",
            "‚úÖ Uploaded: events.out.tfevents.1748213977.eb8dd917cc36.101646.0\n",
            "üì§ Uploading: hparams.yaml (0.0 MB)\n",
            "‚úÖ Uploaded: hparams.yaml\n",
            "üì§ Uploading: checkpoints/epoch=19-step=500.ckpt (16429.4 MB)\n",
            "‚úÖ Uploaded: checkpoints/epoch=19-step=500.ckpt\n",
            "üì§ Uploading: checkpoints/epoch=39-step=1000.ckpt (16429.4 MB)\n",
            "‚úÖ Uploaded: checkpoints/epoch=39-step=1000.ckpt\n",
            "üì§ Uploading: checkpoints/epoch=59-step=1500.ckpt (16429.4 MB)\n",
            "‚úÖ Uploaded: checkpoints/epoch=59-step=1500.ckpt\n",
            "üì§ Uploading: checkpoints/epoch=39-step=1000_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "‚úÖ Uploaded: checkpoints/epoch=39-step=1000_lora/pytorch_lora_weights.safetensors\n",
            "üì§ Uploading: checkpoints/epoch=19-step=500_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "‚úÖ Uploaded: checkpoints/epoch=19-step=500_lora/pytorch_lora_weights.safetensors\n",
            "üì§ Uploading: checkpoints/epoch=59-step=1500_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "‚úÖ Uploaded: checkpoints/epoch=59-step=1500_lora/pytorch_lora_weights.safetensors\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_15_Big_Infinite_For_a_Song_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_15_Big_Infinite_For_a_Song_0.wav\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_15_Big_Infinite_For_a_Song_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_15_Big_Infinite_For_a_Song_0.txt\n",
            "üì§ Uploading: eval_results/step_0/target_wav_12_Thesieryj_Bright_Asian_Style_Pop_Hard_Loop_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_12_Thesieryj_Bright_Asian_Style_Pop_Hard_Loop_0.wav\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_19_Massimo_Kyo_Di_Nocera_Intervallo_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_19_Massimo_Kyo_Di_Nocera_Intervallo_0.txt\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_13_Tes_WITHOUT_CACHE_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_13_Tes_WITHOUT_CACHE_0.wav\n",
            "üì§ Uploading: eval_results/step_0/target_wav_13_Tes_WITHOUT_CACHE_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_13_Tes_WITHOUT_CACHE_0.wav\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_12_Thesieryj_Bright_Asian_Style_Pop_Hard_Loop_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_12_Thesieryj_Bright_Asian_Style_Pop_Hard_Loop_0.wav\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_05_chapardos_111111111_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_05_chapardos_111111111_0.txt\n",
            "üì§ Uploading: eval_results/step_0/target_wav_15_Big_Infinite_For_a_Song_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_15_Big_Infinite_For_a_Song_0.wav\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_05_chapardos_111111111_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_05_chapardos_111111111_0.wav\n",
            "üì§ Uploading: eval_results/step_0/target_wav_05_chapardos_111111111_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_05_chapardos_111111111_0.wav\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_05_JekK_Day_Free_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_05_JekK_Day_Free_0.txt\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_02_Guy_Berrier_REGGmANNFree_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_02_Guy_Berrier_REGGmANNFree_0.txt\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_02_Guy_Berrier_REGGmANNFree_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_02_Guy_Berrier_REGGmANNFree_0.wav\n",
            "üì§ Uploading: eval_results/step_0/target_wav_02_Guy_Berrier_REGGmANNFree_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_02_Guy_Berrier_REGGmANNFree_0.wav\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_13_Tes_WITHOUT_CACHE_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_13_Tes_WITHOUT_CACHE_0.txt\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_12_Thesieryj_Bright_Asian_Style_Pop_Hard_Loop_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_12_Thesieryj_Bright_Asian_Style_Pop_Hard_Loop_0.txt\n",
            "üì§ Uploading: eval_results/step_0/key_prompt_lyric_07_Kenny_Leonore_Lustrous_Longings_of_La_Preneuse_0.txt (0.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/key_prompt_lyric_07_Kenny_Leonore_Lustrous_Longings_of_La_Preneuse_0.txt\n",
            "üì§ Uploading: eval_results/step_0/target_wav_05_JekK_Day_Free_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_05_JekK_Day_Free_0.wav\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_05_JekK_Day_Free_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_05_JekK_Day_Free_0.wav\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_19_Massimo_Kyo_Di_Nocera_Intervallo_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_19_Massimo_Kyo_Di_Nocera_Intervallo_0.wav\n",
            "üì§ Uploading: eval_results/step_0/target_wav_19_Massimo_Kyo_Di_Nocera_Intervallo_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_19_Massimo_Kyo_Di_Nocera_Intervallo_0.wav\n",
            "üì§ Uploading: eval_results/step_0/target_wav_07_Kenny_Leonore_Lustrous_Longings_of_La_Preneuse_0.wav (22.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/target_wav_07_Kenny_Leonore_Lustrous_Longings_of_La_Preneuse_0.wav\n",
            "üì§ Uploading: eval_results/step_0/pred_wav_07_Kenny_Leonore_Lustrous_Longings_of_La_Preneuse_0.wav (1.0 MB)\n",
            "‚úÖ Uploaded: eval_results/step_0/pred_wav_07_Kenny_Leonore_Lustrous_Longings_of_La_Preneuse_0.wav\n",
            "============================================================\n",
            "üìà Upload Summary:\n",
            "   ‚úÖ Successfully uploaded: 32 files\n",
            "   ‚ùå Failed uploads: 0 files\n",
            "   üìç GCS Location: gs://uchicago-bayesian-bayes-beats/experiments/ace_lora_finetune_20250526_023307/\n",
            "\n",
            "üìã Key files uploaded:\n",
            "   ‚Ä¢ checkpoints/epoch=19-step=500.ckpt\n",
            "   ‚Ä¢ checkpoints/epoch=39-step=1000.ckpt\n",
            "   ‚Ä¢ checkpoints/epoch=59-step=1500.ckpt\n",
            "   ‚Ä¢ hparams.yaml\n",
            "   ‚Ä¢ eval_results/step_0/key_prompt_lyric_15_Big_Infinite_For_a_Song_0.txt\n",
            "   ‚Ä¢ eval_results/step_0/key_prompt_lyric_19_Massimo_Kyo_Di_Nocera_Intervallo_0.txt\n",
            "   ‚Ä¢ eval_results/step_0/key_prompt_lyric_05_chapardos_111111111_0.txt\n",
            "   ‚Ä¢ eval_results/step_0/key_prompt_lyric_05_JekK_Day_Free_0.txt\n",
            "   ‚Ä¢ eval_results/step_0/key_prompt_lyric_02_Guy_Berrier_REGGmANNFree_0.txt\n",
            "   ... and 3 more\n",
            "\n",
            "üéâ Upload process completed!\n",
            "üí° You can now access your experiment files from the GCS bucket\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Script to upload entire experiment folder to Google Cloud Storage bucket\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from datetime import datetime\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "BUCKET_NAME = \"uchicago-bayesian-bayes-beats\"\n",
        "LOCAL_EXPERIMENT_PATH = \"/content/exps/logs/lightning_logs/2025-05-25_22-59-37ace_lora_finetune\"\n",
        "\n",
        "def get_all_files(directory):\n",
        "    \"\"\"Recursively get all files in directory\"\"\"\n",
        "    files = []\n",
        "    for root, dirs, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            file_path = os.path.join(root, filename)\n",
        "            files.append(file_path)\n",
        "    return files\n",
        "\n",
        "def upload_folder_to_gcs():\n",
        "    \"\"\"Upload the entire experiment folder to Google Cloud Storage\"\"\"\n",
        "\n",
        "    # Initialize the GCS client\n",
        "    try:\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(BUCKET_NAME)\n",
        "        print(f\"‚úÖ Successfully connected to bucket: {BUCKET_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error connecting to GCS: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Check if local folder exists\n",
        "    if not os.path.exists(LOCAL_EXPERIMENT_PATH):\n",
        "        print(f\"‚ùå Error: Local experiment folder not found at {LOCAL_EXPERIMENT_PATH}\")\n",
        "        return False\n",
        "\n",
        "    # Get all files in the experiment folder\n",
        "    print(f\"üìÅ Scanning folder: {LOCAL_EXPERIMENT_PATH}\")\n",
        "    all_files = get_all_files(LOCAL_EXPERIMENT_PATH)\n",
        "\n",
        "    if not all_files:\n",
        "        print(\"‚ùå No files found in the experiment folder\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üìä Found {len(all_files)} files to upload\")\n",
        "\n",
        "    # Calculate total size\n",
        "    total_size = sum(os.path.getsize(f) for f in all_files if os.path.exists(f))\n",
        "    print(f\"üì¶ Total size: {total_size / (1024**3):.2f} GB\")\n",
        "\n",
        "    # Create destination prefix in bucket\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    destination_prefix = f\"experiments/ace_lora_finetune_{timestamp}\"\n",
        "\n",
        "    print(f\"üöÄ Starting upload to gs://{BUCKET_NAME}/{destination_prefix}/\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    uploaded_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    for file_path in all_files:\n",
        "        try:\n",
        "            # Get relative path from the experiment folder\n",
        "            rel_path = os.path.relpath(file_path, LOCAL_EXPERIMENT_PATH)\n",
        "\n",
        "            # Create destination path in bucket\n",
        "            destination_path = f\"{destination_prefix}/{rel_path}\"\n",
        "\n",
        "            # Get file size\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            size_mb = file_size / (1024**2)\n",
        "\n",
        "            print(f\"üì§ Uploading: {rel_path} ({size_mb:.1f} MB)\")\n",
        "\n",
        "            # Create blob and upload\n",
        "            blob = bucket.blob(destination_path)\n",
        "            blob.upload_from_filename(file_path)\n",
        "\n",
        "            uploaded_count += 1\n",
        "            print(f\"‚úÖ Uploaded: {rel_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_count += 1\n",
        "            print(f\"‚ùå Failed to upload {rel_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìà Upload Summary:\")\n",
        "    print(f\"   ‚úÖ Successfully uploaded: {uploaded_count} files\")\n",
        "    print(f\"   ‚ùå Failed uploads: {failed_count} files\")\n",
        "    print(f\"   üìç GCS Location: gs://{BUCKET_NAME}/{destination_prefix}/\")\n",
        "\n",
        "    # List key files that were uploaded\n",
        "    print(f\"\\nüìã Key files uploaded:\")\n",
        "    key_patterns = ['*.ckpt', '*.yaml', '*.json', '*.log', '*.txt']\n",
        "    for pattern in key_patterns:\n",
        "        matching_files = [f for f in all_files if any(f.endswith(ext) for ext in pattern.replace('*', '').split())]\n",
        "        if matching_files:\n",
        "            for file_path in matching_files[:5]:  # Show first 5 matches\n",
        "                rel_path = os.path.relpath(file_path, LOCAL_EXPERIMENT_PATH)\n",
        "                print(f\"   ‚Ä¢ {rel_path}\")\n",
        "            if len(matching_files) > 5:\n",
        "                print(f\"   ... and {len(matching_files) - 5} more\")\n",
        "\n",
        "    return uploaded_count > 0\n",
        "\n",
        "def main():\n",
        "    print(\"üöÄ Starting full experiment folder upload to Google Cloud Storage...\")\n",
        "    print(f\"üìÇ Local folder: {LOCAL_EXPERIMENT_PATH}\")\n",
        "    print(f\"ü™£ Destination bucket: {BUCKET_NAME}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    success = upload_folder_to_gcs()\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nüéâ Upload process completed!\")\n",
        "        print(\"üí° You can now access your experiment files from the GCS bucket\")\n",
        "    else:\n",
        "        print(\"\\nüí• Upload failed. Please check the error messages above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIVvrzwtb8Z4"
      },
      "outputs": [],
      "source": [
        "torch_compile = True # @param {type: \"boolean\"}\n",
        "cpu_offload = False # @param {type: \"boolean\"}\n",
        "overlapped_decode = True # @param {type: \"boolean\"}\n",
        "#bf16 = True # @param {type: \"boolean\"}\n",
        "\n",
        "!acestep --checkpoint_path /unzip/checkpoints/ --port 7865 --device_id 0 --share true --torch_compile {torch_compile} --cpu_offload {cpu_offload} --overlapped_decode {overlapped_decode}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-eqSIaI9W9Z",
        "outputId": "f0102994-6eb7-403a-9199-758ecf0d4017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting FULL FOLDER download from Google Cloud Storage...\n",
            "ü™£ Source bucket: uchicago-bayesian-bayes-beats\n",
            "üìÇ Target folder: experiments/ace_lora_finetune_20250526_023307/checkpoints\n",
            "üìÇ Download location: /content/checkpoints\n",
            "üí™ Leveraging A100 GPU high-bandwidth connection!\n",
            "------------------------------------------------------------\n",
            "‚úÖ Successfully connected to bucket: uchicago-bayesian-bayes-beats\n",
            "üìÇ Downloading entire folder: experiments/ace_lora_finetune_20250526_023307/checkpoints\n",
            "üîç Scanning folder contents...\n",
            "üéØ Found 6 files to download\n",
            "üì¶ Total size: 48.32 GB\n",
            "üìä File types:\n",
            "   ‚Ä¢ .ckpt: 3 files\n",
            "   ‚Ä¢ .safetensors: 3 files\n",
            "\n",
            "üöÄ Starting parallel download to /content/checkpoints/\n",
            "üöÑ Using A100 GPU's high-bandwidth for faster downloads!\n",
            "============================================================\n",
            "‚úÖ epoch=19-step=500_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "‚úÖ epoch=39-step=1000_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "‚úÖ epoch=59-step=1500_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "‚úÖ epoch=39-step=1000.ckpt (16429.4 MB)\n",
            "‚úÖ epoch=19-step=500.ckpt (16429.4 MB)\n",
            "‚úÖ epoch=59-step=1500.ckpt (16429.4 MB)\n",
            "============================================================\n",
            "üìà Download Summary:\n",
            "   ‚úÖ Successfully downloaded: 6 files\n",
            "   ‚ùå Failed downloads: 0 files\n",
            "   üìç Local location: /content/checkpoints/\n",
            "\n",
            "üìã Downloaded files:\n",
            "   ‚Ä¢ epoch=19-step=500.ckpt (16429.4 MB)\n",
            "   ‚Ä¢ epoch=39-step=1000.ckpt (16429.4 MB)\n",
            "   ‚Ä¢ epoch=59-step=1500.ckpt (16429.4 MB)\n",
            "   ‚Ä¢ epoch=39-step=1000_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "   ‚Ä¢ epoch=19-step=500_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "   ‚Ä¢ epoch=59-step=1500_lora/pytorch_lora_weights.safetensors (62.5 MB)\n",
            "\n",
            "üéâ Full folder download completed successfully!\n",
            "üí° All checkpoint files and configs are now available in /content/checkpoints/\n",
            "üîß You can now use the directory with:\n",
            "   acestep --checkpoint_path /content/checkpoints/ --port 7865\n",
            "üìÅ This should auto-select the best checkpoint and load configs!\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Script to download checkpoint files from Google Cloud Storage to local content folder\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.cloud import storage\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "BUCKET_NAME = \"uchicago-bayesian-bayes-beats\"\n",
        "LOCAL_DOWNLOAD_PATH = \"/content/checkpoints\"  # Where to save in your content folder\n",
        "\n",
        "def list_experiment_folders(bucket):\n",
        "    \"\"\"List all experiment folders in the bucket\"\"\"\n",
        "    print(\"üìã Available experiment folders:\")\n",
        "    blobs = bucket.list_blobs(prefix=\"experiments/\")\n",
        "    folders = set()\n",
        "\n",
        "    for blob in blobs:\n",
        "        # Extract folder name from blob path\n",
        "        parts = blob.name.split('/')\n",
        "        if len(parts) >= 2:\n",
        "            folder_name = '/'.join(parts[:2])  # experiments/folder_name\n",
        "            folders.add(folder_name)\n",
        "\n",
        "    for i, folder in enumerate(sorted(folders), 1):\n",
        "        print(f\"   {i}. {folder}\")\n",
        "\n",
        "    return sorted(folders)\n",
        "\n",
        "def download_entire_folder(specific_folder=\"experiments/ace_lora_finetune_20250526_023307/checkpoints\"):\n",
        "    \"\"\"Download entire checkpoint folder from Google Cloud Storage with parallel downloads\"\"\"\n",
        "    import concurrent.futures\n",
        "    from threading import Lock\n",
        "\n",
        "    # Initialize the GCS client\n",
        "    try:\n",
        "        client = storage.Client()\n",
        "        bucket = client.bucket(BUCKET_NAME)\n",
        "        print(f\"‚úÖ Successfully connected to bucket: {BUCKET_NAME}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error connecting to GCS: {e}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"üìÇ Downloading entire folder: {specific_folder}\")\n",
        "\n",
        "    # Create local download directory\n",
        "    os.makedirs(LOCAL_DOWNLOAD_PATH, exist_ok=True)\n",
        "\n",
        "    # List ALL files in the checkpoint folder (not just .ckpt files)\n",
        "    print(\"üîç Scanning folder contents...\")\n",
        "    all_blobs = list(bucket.list_blobs(prefix=specific_folder))\n",
        "\n",
        "    # Filter out directory markers and get actual files\n",
        "    file_blobs = [blob for blob in all_blobs if not blob.name.endswith('/') and blob.size > 0]\n",
        "\n",
        "    if not file_blobs:\n",
        "        print(f\"‚ùå No files found in {specific_folder}\")\n",
        "        return False\n",
        "\n",
        "    # Calculate total size\n",
        "    total_size = sum(blob.size for blob in file_blobs)\n",
        "    total_size_gb = total_size / (1024**3)\n",
        "\n",
        "    print(f\"üéØ Found {len(file_blobs)} files to download\")\n",
        "    print(f\"üì¶ Total size: {total_size_gb:.2f} GB\")\n",
        "\n",
        "    # Show file breakdown\n",
        "    file_types = {}\n",
        "    for blob in file_blobs:\n",
        "        ext = os.path.splitext(blob.name)[1] or 'no_ext'\n",
        "        file_types[ext] = file_types.get(ext, 0) + 1\n",
        "\n",
        "    print(\"üìä File types:\")\n",
        "    for ext, count in file_types.items():\n",
        "        print(f\"   ‚Ä¢ {ext}: {count} files\")\n",
        "\n",
        "    print(f\"\\nüöÄ Starting parallel download to {LOCAL_DOWNLOAD_PATH}/\")\n",
        "    print(\"üöÑ Using A100 GPU's high-bandwidth for faster downloads!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Parallel download function\n",
        "    def download_file(blob):\n",
        "        try:\n",
        "            # Preserve folder structure\n",
        "            relative_path = blob.name.replace(specific_folder, '').lstrip('/')\n",
        "            local_path = os.path.join(LOCAL_DOWNLOAD_PATH, relative_path)\n",
        "\n",
        "            # Create subdirectories if needed\n",
        "            local_dir = os.path.dirname(local_path)\n",
        "            if local_dir:\n",
        "                os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "            # Download file\n",
        "            blob.download_to_filename(local_path)\n",
        "\n",
        "            # Verify download\n",
        "            if os.path.exists(local_path):\n",
        "                file_size_mb = os.path.getsize(local_path) / (1024**2)\n",
        "                return f\"‚úÖ {relative_path} ({file_size_mb:.1f} MB)\"\n",
        "            else:\n",
        "                return f\"‚ùå Failed: {relative_path}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error downloading {blob.name}: {e}\"\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel downloads\n",
        "    # A100 has high memory bandwidth, so we can handle many concurrent downloads\n",
        "    downloaded_count = 0\n",
        "    failed_count = 0\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
        "        # Submit all download tasks\n",
        "        future_to_blob = {executor.submit(download_file, blob): blob for blob in file_blobs}\n",
        "\n",
        "        # Process completed downloads\n",
        "        for future in concurrent.futures.as_completed(future_to_blob):\n",
        "            result = future.result()\n",
        "            print(result)\n",
        "\n",
        "            if result.startswith(\"‚úÖ\"):\n",
        "                downloaded_count += 1\n",
        "            else:\n",
        "                failed_count += 1\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"üìà Download Summary:\")\n",
        "    print(f\"   ‚úÖ Successfully downloaded: {downloaded_count} files\")\n",
        "    print(f\"   ‚ùå Failed downloads: {failed_count} files\")\n",
        "    print(f\"   üìç Local location: {LOCAL_DOWNLOAD_PATH}/\")\n",
        "\n",
        "    # List all downloaded files\n",
        "    if downloaded_count > 0:\n",
        "        print(f\"\\nüìã Downloaded files:\")\n",
        "        for root, dirs, files in os.walk(LOCAL_DOWNLOAD_PATH):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                rel_path = os.path.relpath(full_path, LOCAL_DOWNLOAD_PATH)\n",
        "                size_mb = os.path.getsize(full_path) / (1024**2)\n",
        "                print(f\"   ‚Ä¢ {rel_path} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    return downloaded_count > 0\n",
        "\n",
        "def main():\n",
        "    print(\"üöÄ Starting FULL FOLDER download from Google Cloud Storage...\")\n",
        "    print(f\"ü™£ Source bucket: {BUCKET_NAME}\")\n",
        "    print(f\"üìÇ Target folder: experiments/ace_lora_finetune_20250526_023307/checkpoints\")\n",
        "    print(f\"üìÇ Download location: {LOCAL_DOWNLOAD_PATH}\")\n",
        "    print(\"üí™ Leveraging A100 GPU high-bandwidth connection!\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    success = download_entire_folder()\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nüéâ Full folder download completed successfully!\")\n",
        "        print(f\"üí° All checkpoint files and configs are now available in {LOCAL_DOWNLOAD_PATH}/\")\n",
        "        print(f\"üîß You can now use the directory with:\")\n",
        "        print(f\"   acestep --checkpoint_path {LOCAL_DOWNLOAD_PATH}/ --port 7865\")\n",
        "        print(\"üìÅ This should auto-select the best checkpoint and load configs!\")\n",
        "    else:\n",
        "        print(\"\\nüí• Download failed. Please check the error messages above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94mGGl9uEnvY"
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
        "!export CUDA_LAUNCH_BLOCKING=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoxDNGUw_4mB",
        "outputId": "eaa2867d-455f-4600-f535-6a8e1ff1e249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-26 19:30:22.927490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748287822.949375   12550 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748287822.955909   12550 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "* Running on local URL:  http://127.0.0.1:7865\n",
            "* Running on public URL: https://197fbfaf148b99a4b7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "\u001b[32m2025-05-26 19:31:47.116\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1477\u001b[0m - \u001b[33m\u001b[1mCheckpoint not loaded, loading checkpoint...\u001b[0m\n",
            "\u001b[32m2025-05-26 19:31:47.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mget_checkpoint_path\u001b[0m:\u001b[36m178\u001b[0m - \u001b[1mDownload models from Hugging Face: ACE-Step/ACE-Step-v1-3.5B, cache to: /unzip/checkpoints/\u001b[0m\n",
            "Fetching 14 files:   0% 0/14 [00:00<?, ?it/s]\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/314M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/6.61G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:   7% 21.0M/314M [00:00<00:02, 139MB/s]\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 21.0M/6.61G [00:00<00:46, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "config.json: 100% 639/639 [00:00<00:00, 3.44MB/s]\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 940/940 [00:00<00:00, 5.17MB/s]\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 1.03k/1.03k [00:00<00:00, 9.72MB/s]\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 100% 1.58k/1.58k [00:00<00:00, 13.7MB/s]\n",
            "Fetching 14 files:   7% 1/14 [00:00<00:06,  2.03it/s]\n",
            "\n",
            "\n",
            "README.md: 100% 3.51k/3.51k [00:00<00:00, 34.0MB/s]\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 52.4M/314M [00:00<00:01, 211MB/s]\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   1% 52.4M/6.61G [00:00<00:30, 214MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:  33% 105M/314M [00:00<00:00, 314MB/s] \u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 105M/6.61G [00:00<00:20, 317MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   0% 0.00/206M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:   0% 0.00/1.13G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/16.8M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   2% 147M/6.61G [00:00<00:20, 308MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:  47% 147M/314M [00:00<00:00, 297MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 10.5M/206M [00:00<00:01, 99.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:   2% 21.0M/1.13G [00:00<00:08, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 16.8M/16.8M [00:00<00:00, 135MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "special_tokens_map.json: 100% 7.08k/7.08k [00:00<00:00, 28.6MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 100% 61.7k/61.7k [00:00<00:00, 45.7MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "config.json: 100% 848/848 [00:00<00:00, 6.19MB/s]\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 178M/314M [00:00<00:00, 247MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 31.5M/206M [00:00<00:01, 123MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 189M/6.61G [00:00<00:25, 253MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:   5% 52.4M/1.13G [00:00<00:05, 187MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:  67% 210M/314M [00:00<00:00, 253MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 62.9M/206M [00:00<00:00, 182MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   3% 220M/6.61G [00:00<00:23, 267MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:   7% 83.9M/1.13G [00:00<00:04, 217MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:  77% 241M/314M [00:00<00:00, 259MB/s]\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  46% 94.4M/206M [00:00<00:00, 214MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 252M/6.61G [00:00<00:23, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  10% 115M/1.13G [00:00<00:04, 230MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 126M/206M [00:00<00:00, 240MB/s] \u001b[A\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:  87% 273M/314M [00:01<00:00, 259MB/s]\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   4% 283M/6.61G [00:01<00:23, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  13% 147M/1.13G [00:00<00:04, 244MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 157M/206M [00:00<00:00, 257MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 315M/6.61G [00:01<00:23, 273MB/s]\u001b[A\u001b[A\n",
            "diffusion_pytorch_model.safetensors:  97% 304M/314M [00:01<00:00, 246MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 314M/314M [00:01<00:00, 252MB/s]\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 189M/206M [00:00<00:00, 250MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   5% 346M/6.61G [00:01<00:23, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 206M/206M [00:00<00:00, 224MB/s]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   6% 388M/6.61G [00:01<00:20, 300MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  23% 262M/1.13G [00:01<00:02, 313MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 430M/6.61G [00:01<00:18, 326MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  27% 304M/1.13G [00:01<00:02, 319MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   7% 472M/6.61G [00:01<00:17, 351MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  31% 346M/1.13G [00:01<00:02, 306MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 514M/6.61G [00:01<00:18, 334MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  34% 388M/1.13G [00:01<00:02, 323MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   8% 556M/6.61G [00:01<00:17, 339MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  38% 430M/1.13G [00:01<00:02, 348MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:   9% 598M/6.61G [00:01<00:16, 359MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  42% 472M/1.13G [00:01<00:01, 360MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 650M/6.61G [00:02<00:15, 380MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  47% 524M/1.13G [00:01<00:01, 382MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  10% 692M/6.61G [00:02<00:15, 384MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  50% 566M/1.13G [00:04<00:12, 44.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  11% 734M/6.61G [00:05<02:13, 44.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  54% 608M/1.13G [00:04<00:08, 60.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 776M/6.61G [00:05<01:37, 59.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  58% 650M/1.13G [00:04<00:05, 80.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  12% 818M/6.61G [00:05<01:12, 79.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  61% 692M/1.13G [00:05<00:04, 105MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  13% 870M/6.61G [00:05<00:51, 111MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  65% 734M/1.13G [00:05<00:02, 134MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  14% 923M/6.61G [00:05<00:38, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  69% 776M/1.13G [00:05<00:02, 168MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  15% 975M/6.61G [00:05<00:30, 187MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  73% 828M/1.13G [00:05<00:01, 212MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 1.03G/6.61G [00:05<00:24, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  77% 870M/1.13G [00:05<00:01, 247MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  16% 1.07G/6.61G [00:05<00:21, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  81% 912M/1.13G [00:05<00:00, 279MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.11G/6.61G [00:06<00:19, 288MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  86% 965M/1.13G [00:05<00:00, 315MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  17% 1.15G/6.61G [00:06<00:17, 314MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  89% 1.01G/1.13G [00:05<00:00, 338MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  18% 1.21G/6.61G [00:06<00:15, 343MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  93% 1.05G/1.13G [00:05<00:00, 357MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  19% 1.25G/6.61G [00:06<00:15, 351MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model.safetensors:  97% 1.09G/1.13G [00:06<00:00, 365MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model.safetensors: 100% 1.13G/1.13G [00:06<00:00, 185MB/s]\n",
            "\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  21% 1.36G/6.61G [00:06<00:12, 420MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 1.43G/6.61G [00:06<00:11, 457MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  22% 1.48G/6.61G [00:06<00:13, 386MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  23% 1.52G/6.61G [00:07<00:15, 325MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 1.56G/6.61G [00:07<00:17, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  24% 1.59G/6.61G [00:07<00:20, 240MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 1.63G/6.61G [00:07<00:21, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  25% 1.66G/6.61G [00:07<00:21, 235MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 1.69G/6.61G [00:07<00:19, 251MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  26% 1.73G/6.61G [00:08<00:17, 276MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 1.77G/6.61G [00:08<00:16, 295MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  27% 1.81G/6.61G [00:08<00:14, 321MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  28% 1.86G/6.61G [00:08<00:14, 329MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 1.90G/6.61G [00:08<00:13, 346MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  29% 1.95G/6.61G [00:08<00:12, 369MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  30% 1.99G/6.61G [00:08<00:12, 376MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  31% 2.03G/6.61G [00:08<00:11, 384MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 2.09G/6.61G [00:08<00:11, 398MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  32% 2.14G/6.61G [00:09<00:11, 406MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  33% 2.19G/6.61G [00:09<00:10, 412MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  34% 2.24G/6.61G [00:09<00:10, 416MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 2.30G/6.61G [00:09<00:10, 419MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  35% 2.34G/6.61G [00:09<00:10, 418MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  36% 2.39G/6.61G [00:09<00:09, 423MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  37% 2.44G/6.61G [00:09<00:09, 426MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  38% 2.50G/6.61G [00:09<00:09, 443MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  39% 2.55G/6.61G [00:10<00:08, 452MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  39% 2.61G/6.61G [00:10<00:08, 480MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  40% 2.66G/6.61G [00:10<00:12, 314MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  41% 2.73G/6.61G [00:10<00:10, 364MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  42% 2.78G/6.61G [00:10<00:09, 393MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  43% 2.83G/6.61G [00:10<00:08, 422MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  44% 2.88G/6.61G [00:10<00:08, 426MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 2.95G/6.61G [00:11<00:08, 455MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  45% 3.00G/6.61G [00:11<00:07, 472MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  46% 3.06G/6.61G [00:11<00:07, 492MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  47% 3.12G/6.61G [00:11<00:06, 503MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  48% 3.19G/6.61G [00:11<00:06, 512MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  49% 3.25G/6.61G [00:11<00:06, 520MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  50% 3.31G/6.61G [00:11<00:06, 522MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  51% 3.38G/6.61G [00:11<00:06, 518MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  52% 3.43G/6.61G [00:11<00:06, 470MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 3.48G/6.61G [00:12<00:07, 441MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  53% 3.53G/6.61G [00:12<00:07, 426MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  54% 3.59G/6.61G [00:12<00:07, 381MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  55% 3.63G/6.61G [00:12<00:07, 383MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 3.67G/6.61G [00:12<00:07, 370MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  56% 3.72G/6.61G [00:12<00:07, 369MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  57% 3.76G/6.61G [00:12<00:07, 367MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 3.81G/6.61G [00:13<00:07, 378MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  58% 3.85G/6.61G [00:13<00:07, 377MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 3.89G/6.61G [00:13<00:07, 381MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  59% 3.93G/6.61G [00:13<00:11, 234MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  60% 3.96G/6.61G [00:15<00:50, 52.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  61% 4.03G/6.61G [00:15<00:31, 82.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  62% 4.09G/6.61G [00:15<00:21, 119MB/s] \u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  63% 4.15G/6.61G [00:16<00:15, 163MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 4.20G/6.61G [00:16<00:11, 202MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  64% 4.26G/6.61G [00:16<00:09, 244MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  65% 4.31G/6.61G [00:16<00:08, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  66% 4.36G/6.61G [00:16<00:06, 325MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  67% 4.41G/6.61G [00:16<00:06, 364MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 4.47G/6.61G [00:16<00:05, 391MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  68% 4.52G/6.61G [00:16<00:05, 415MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  69% 4.57G/6.61G [00:16<00:04, 430MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  70% 4.62G/6.61G [00:17<00:04, 442MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  71% 4.68G/6.61G [00:17<00:04, 449MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 4.73G/6.61G [00:17<00:04, 459MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  72% 4.78G/6.61G [00:17<00:03, 468MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  73% 4.83G/6.61G [00:17<00:03, 477MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  74% 4.89G/6.61G [00:17<00:03, 484MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 4.94G/6.61G [00:17<00:03, 488MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  75% 4.99G/6.61G [00:17<00:03, 489MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  76% 5.04G/6.61G [00:17<00:03, 488MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  77% 5.10G/6.61G [00:17<00:03, 487MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  78% 5.15G/6.61G [00:18<00:03, 487MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 5.20G/6.61G [00:18<00:02, 487MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  79% 5.25G/6.61G [00:18<00:02, 488MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  80% 5.31G/6.61G [00:18<00:02, 481MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  81% 5.36G/6.61G [00:18<00:02, 478MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  82% 5.41G/6.61G [00:18<00:02, 480MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  83% 5.46G/6.61G [00:18<00:02, 481MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  83% 5.52G/6.61G [00:18<00:02, 470MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  84% 5.57G/6.61G [00:18<00:02, 470MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  85% 5.62G/6.61G [00:19<00:02, 477MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  86% 5.67G/6.61G [00:19<00:01, 477MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  87% 5.73G/6.61G [00:19<00:01, 478MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  87% 5.78G/6.61G [00:19<00:01, 479MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  88% 5.83G/6.61G [00:19<00:01, 480MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  89% 5.88G/6.61G [00:19<00:01, 484MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  90% 5.93G/6.61G [00:19<00:01, 487MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 5.99G/6.61G [00:19<00:01, 493MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  91% 6.04G/6.61G [00:19<00:01, 495MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  92% 6.09G/6.61G [00:20<00:01, 494MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  93% 6.14G/6.61G [00:20<00:00, 499MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  94% 6.20G/6.61G [00:20<00:00, 501MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 6.25G/6.61G [00:20<00:00, 500MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  95% 6.30G/6.61G [00:20<00:00, 498MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  96% 6.35G/6.61G [00:20<00:00, 318MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  97% 6.41G/6.61G [00:20<00:00, 355MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 6.46G/6.61G [00:20<00:00, 384MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors:  98% 6.51G/6.61G [00:21<00:00, 412MB/s]\u001b[A\u001b[A\n",
            "\n",
            "diffusion_pytorch_model.safetensors: 100% 6.61G/6.61G [00:21<00:00, 310MB/s]\n",
            "Fetching 14 files: 100% 14/14 [00:21<00:00,  1.54s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n",
            "\u001b[32m2025-05-26 19:32:23.287\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 36.17 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:32:46.301\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:32:46.342\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:08<00:00,  7.28it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:32:55.422\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:32:55.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526193255_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00,  9.00it/s]\n",
            "\u001b[32m2025-05-26 19:32:55.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.83GB reserved\u001b[0m\n",
            "\u001b[32m2025-05-26 19:44:26.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:44:55.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:44:55.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  7.90it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:45:03.370\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:45:03.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526194503_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 19.82it/s]\n",
            "\u001b[32m2025-05-26 19:45:03.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.44GB allocated, 7.83GB reserved\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "\u001b[32m2025-05-26 19:47:12.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:47:13.002\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:47:13.004\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  7.77it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:47:21.102\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:47:21.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526194721_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 15.50it/s]\n",
            "\u001b[32m2025-05-26 19:47:21.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.88GB reserved\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "\u001b[32m2025-05-26 19:51:19.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:51:19.929\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:51:19.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  7.81it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:51:28.006\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:51:28.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526195128_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 16.08it/s]\n",
            "\u001b[32m2025-05-26 19:51:28.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.83GB reserved\u001b[0m\n",
            "\u001b[32m2025-05-26 19:54:26.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:54:26.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:54:26.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  7.81it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:54:34.837\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:54:34.837\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526195434_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 16.31it/s]\n",
            "\u001b[32m2025-05-26 19:54:34.902\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.82GB reserved\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2191, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1702, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 894, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/ACE-Step/acestep/ui/components.py\", line 928, in load_data\n",
            "    json_file = os.path.join(output_file_dir, json_file)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 90, in join\n",
            "  File \"<frozen genericpath>\", line 152, in _check_arg_types\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n",
            "\u001b[32m2025-05-26 19:56:00.782\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:00.795\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1533\u001b[0m - \u001b[1mrandom audio duration: 201.11963833301732\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:00.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:00.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:15<00:00,  3.81it/s]\n",
            "Using Overlapped DCAE and Vocoder\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:56:23.702\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:23.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526195623_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00,  2.27it/s]\n",
            "\u001b[32m2025-05-26 19:56:24.149\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.85GB reserved\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:45.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:45.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:45.176\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  7.76it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:56:53.259\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:56:53.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526195653_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 20.84it/s]\n",
            "\u001b[32m2025-05-26 19:56:53.310\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.81GB reserved\u001b[0m\n",
            "\u001b[32m2025-05-26 19:58:42.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:58:42.311\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:58:42.313\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  8.22it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:58:49.954\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:58:49.954\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526195849_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 22.20it/s]\n",
            "\u001b[32m2025-05-26 19:58:50.001\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.44GB allocated, 7.83GB reserved\u001b[0m\n",
            "\u001b[32m2025-05-26 19:59:36.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 19:59:36.512\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 19:59:36.514\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  8.18it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 19:59:44.161\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 19:59:44.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526195944_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 20.97it/s]\n",
            "\u001b[32m2025-05-26 19:59:44.212\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.43GB allocated, 7.82GB reserved\u001b[0m\n",
            "\u001b[32m2025-05-26 20:00:01.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m1485\u001b[0m - \u001b[1mModel loaded in 0.00 seconds.\u001b[0m\n",
            "\u001b[32m2025-05-26 20:00:01.932\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m847\u001b[0m - \u001b[1mcfg_type: apg, guidance_scale: 15, omega_scale: 10\u001b[0m\n",
            "\u001b[32m2025-05-26 20:00:01.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mtext2music_diffusion_process\u001b[0m:\u001b[36m1072\u001b[0m - \u001b[1mstart_idx: 15, end_idx: 45, num_inference_steps: 60\u001b[0m\n",
            "100% 60/60 [00:07<00:00,  8.18it/s]\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[32m2025-05-26 20:00:09.572\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1381\u001b[0m - \u001b[33m\u001b[1msave_path is None, using default path ./outputs/\u001b[0m\n",
            "\u001b[32m2025-05-26 20:00:09.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36msave_wav_file\u001b[0m:\u001b[36m1396\u001b[0m - \u001b[1mSaving audio to ./outputs/output_20250526200009_0.wav\u001b[0m\n",
            "100% 1/1 [00:00<00:00, 22.25it/s]\n",
            "\u001b[32m2025-05-26 20:00:09.620\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36macestep.pipeline_ace_step\u001b[0m:\u001b[36mcleanup_memory\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mGPU Memory: 7.44GB allocated, 7.82GB reserved\u001b[0m\n",
            "Keyboard interruption in main thread... closing server.\n",
            "\n",
            "Aborted!\n",
            "Killing tunnel 127.0.0.1:7865 <> https://197fbfaf148b99a4b7.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!acestep --checkpoint_path /unzip/checkpoints/ --port 7865 --device_id 0 --share true --torch_compile true --cpu_offload false --overlapped_decode true"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08cd55b7c44c4109ad39f3e0b75e4e15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c92605dfe99d45aba79963c52a5ad36f",
              "IPY_MODEL_69a052056d9f4f9ebbf1737cbd20b83d",
              "IPY_MODEL_7d3c6f55ced347b6b87c2949994f1cdf"
            ],
            "layout": "IPY_MODEL_f303235752c24851bc34874c2ef86512"
          }
        },
        "1003a56e2f7c44848ee5a8970261c108": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d39be46280643a2a92115d3d07d18cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e060a11a2d9471aa8d3c4b22e17405e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23461e7b5f384e5fa36dd2b9cc0562d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69a052056d9f4f9ebbf1737cbd20b83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b130c3ca95014357847ba54d4e5ecf87",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e060a11a2d9471aa8d3c4b22e17405e",
            "value": 200
          }
        },
        "7d3c6f55ced347b6b87c2949994f1cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d39be46280643a2a92115d3d07d18cf",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_23461e7b5f384e5fa36dd2b9cc0562d7",
            "value": "‚Äá200/200‚Äá[00:00&lt;00:00,‚Äá9975.28‚Äáexamples/s]"
          }
        },
        "92ff55892c274304844c50a757b91902": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b130c3ca95014357847ba54d4e5ecf87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92605dfe99d45aba79963c52a5ad36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1003a56e2f7c44848ee5a8970261c108",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_92ff55892c274304844c50a757b91902",
            "value": "Saving‚Äáthe‚Äádataset‚Äá(1/1‚Äáshards):‚Äá100%"
          }
        },
        "f303235752c24851bc34874c2ef86512": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
