{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. Load ace step following the instructions in the README.\n",
    "2. Run the following code: "
   ],
   "id": "145d82d4bc93c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "#from openai import OpenAI\n",
    "from google.cloud import storage\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Import necessary libraries for authentication\n",
    "from google.colab import auth\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Authenticate with Google Cloud\n",
    "print(\"🔐 Authenticating with Google Cloud...\")\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Set up your specific Google Cloud projectr project ID from the screenshot\n",
    "project_id = \"bayes-beats\""
   ],
   "id": "e012ba5ca924f8cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to download checkpoint files from Google Cloud Storage to local content folder\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = \"uchicago-bayesian-bayes-beats\"\n",
    "LOCAL_DOWNLOAD_PATH = \"/content/checkpoints\"  # Where to save in your content folder\n",
    "\n",
    "def list_experiment_folders(bucket):\n",
    "    \"\"\"List all experiment folders in the bucket\"\"\"\n",
    "    print(\"📋 Available experiment folders:\")\n",
    "    blobs = bucket.list_blobs(prefix=\"experiments/\")\n",
    "    folders = set()\n",
    "    \n",
    "    for blob in blobs:\n",
    "        # Extract folder name from blob path\n",
    "        parts = blob.name.split('/')\n",
    "        if len(parts) >= 2:\n",
    "            folder_name = '/'.join(parts[:2])  # experiments/folder_name\n",
    "            folders.add(folder_name)\n",
    "    \n",
    "    for i, folder in enumerate(sorted(folders), 1):\n",
    "        print(f\"   {i}. {folder}\")\n",
    "    \n",
    "    return sorted(folders)\n",
    "\n",
    "def download_entire_folder(specific_folder=\"experiments/ace_lora_finetune_20250526_023307/checkpoints\"):\n",
    "    \"\"\"Download entire checkpoint folder from Google Cloud Storage with parallel downloads\"\"\"\n",
    "    import concurrent.futures\n",
    "    from threading import Lock\n",
    "    \n",
    "    # Initialize the GCS client\n",
    "    try:\n",
    "        client = storage.Client()\n",
    "        bucket = client.bucket(BUCKET_NAME)\n",
    "        print(f\"✅ Successfully connected to bucket: {BUCKET_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error connecting to GCS: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"📂 Downloading entire folder: {specific_folder}\")\n",
    "\n",
    "    # Create local download directory\n",
    "    os.makedirs(LOCAL_DOWNLOAD_PATH, exist_ok=True)\n",
    "    \n",
    "    # List ALL files in the checkpoint folder (not just .ckpt files)\n",
    "    print(\"🔍 Scanning folder contents...\")\n",
    "    all_blobs = list(bucket.list_blobs(prefix=specific_folder))\n",
    "    \n",
    "    # Filter out directory markers and get actual files\n",
    "    file_blobs = [blob for blob in all_blobs if not blob.name.endswith('/') and blob.size > 0]\n",
    "    \n",
    "    if not file_blobs:\n",
    "        print(f\"❌ No files found in {specific_folder}\")\n",
    "        return False\n",
    "    \n",
    "    # Calculate total size\n",
    "    total_size = sum(blob.size for blob in file_blobs)\n",
    "    total_size_gb = total_size / (1024**3)\n",
    "    \n",
    "    print(f\"🎯 Found {len(file_blobs)} files to download\")\n",
    "    print(f\"📦 Total size: {total_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Show file breakdown\n",
    "    file_types = {}\n",
    "    for blob in file_blobs:\n",
    "        ext = os.path.splitext(blob.name)[1] or 'no_ext'\n",
    "        file_types[ext] = file_types.get(ext, 0) + 1\n",
    "    \n",
    "    print(\"📊 File types:\")\n",
    "    for ext, count in file_types.items():\n",
    "        print(f\"   • {ext}: {count} files\")\n",
    "    \n",
    "    print(f\"\\n🚀 Starting parallel download to {LOCAL_DOWNLOAD_PATH}/\")\n",
    "    print(\"🚄 Using A100 GPU's high-bandwidth for faster downloads!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Parallel download function\n",
    "    def download_file(blob):\n",
    "        try:\n",
    "            # Preserve folder structure\n",
    "            relative_path = blob.name.replace(specific_folder, '').lstrip('/')\n",
    "            local_path = os.path.join(LOCAL_DOWNLOAD_PATH, relative_path)\n",
    "            \n",
    "            # Create subdirectories if needed\n",
    "            local_dir = os.path.dirname(local_path)\n",
    "            if local_dir:\n",
    "                os.makedirs(local_dir, exist_ok=True)\n",
    "            \n",
    "            # Download file\n",
    "            blob.download_to_filename(local_path)\n",
    "            \n",
    "            # Verify download\n",
    "            if os.path.exists(local_path):\n",
    "                file_size_mb = os.path.getsize(local_path) / (1024**2)\n",
    "                return f\"✅ {relative_path} ({file_size_mb:.1f} MB)\"\n",
    "            else:\n",
    "                return f\"❌ Failed: {relative_path}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"❌ Error downloading {blob.name}: {e}\"\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel downloads\n",
    "    # A100 has high memory bandwidth, so we can handle many concurrent downloads\n",
    "    downloaded_count = 0\n",
    "    failed_count = 0\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        # Submit all download tasks\n",
    "        future_to_blob = {executor.submit(download_file, blob): blob for blob in file_blobs}\n",
    "        \n",
    "        # Process completed downloads\n",
    "        for future in concurrent.futures.as_completed(future_to_blob):\n",
    "            result = future.result()\n",
    "            print(result)\n",
    "            \n",
    "            if result.startswith(\"✅\"):\n",
    "                downloaded_count += 1\n",
    "            else:\n",
    "                failed_count += 1\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"📈 Download Summary:\")\n",
    "    print(f\"   ✅ Successfully downloaded: {downloaded_count} files\")\n",
    "    print(f\"   ❌ Failed downloads: {failed_count} files\")\n",
    "    print(f\"   📍 Local location: {LOCAL_DOWNLOAD_PATH}/\")\n",
    "    \n",
    "    # List all downloaded files\n",
    "    if downloaded_count > 0:\n",
    "        print(f\"\\n📋 Downloaded files:\")\n",
    "        for root, dirs, files in os.walk(LOCAL_DOWNLOAD_PATH):\n",
    "            for file in files:\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, LOCAL_DOWNLOAD_PATH)\n",
    "                size_mb = os.path.getsize(full_path) / (1024**2)\n",
    "                print(f\"   • {rel_path} ({size_mb:.1f} MB)\")\n",
    "    \n",
    "    return downloaded_count > 0\n",
    "\n",
    "def main():\n",
    "    print(\"🚀 Starting FULL FOLDER download from Google Cloud Storage...\")\n",
    "    print(f\"🪣 Source bucket: {BUCKET_NAME}\")\n",
    "    print(f\"📂 Target folder: experiments/ace_lora_finetune_20250526_023307/checkpoints\")\n",
    "    print(f\"📂 Download location: {LOCAL_DOWNLOAD_PATH}\")\n",
    "    print(\"💪 Leveraging A100 GPU high-bandwidth connection!\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    success = download_entire_folder()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 Full folder download completed successfully!\")\n",
    "        print(f\"💡 All checkpoint files and configs are now available in {LOCAL_DOWNLOAD_PATH}/\")\n",
    "        print(f\"🔧 You can now use the directory with:\")\n",
    "        print(f\"   acestep --checkpoint_path {LOCAL_DOWNLOAD_PATH}/ --port 7865\")\n",
    "        print(\"📁 This should auto-select the best checkpoint and load configs!\")\n",
    "    else:\n",
    "        print(\"\\n💥 Download failed. Please check the error messages above.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## IMPORTANT: put the checkpoints folder in a directory called \"unzip\" \n",
    "Your /content directory should look like this:\n",
    ".\n",
    "├── ACE-Step/\n",
    "├── outputs/\n",
    "├── sample_data/\n",
    "├── unzip/\n",
    "│   └── checkpoints/\n",
    "\n",
    "(click this cell to see this in a more intuitive way)\n",
    "\n",
    "Run the following: "
   ],
   "id": "7cb97f3ce01bd031"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "!export CUDA_LAUNCH_BLOCKING=0"
   ],
   "id": "661e4e950ed2e3af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Choose the public URL below:",
   "id": "743c194a20caeca3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!acestep --checkpoint_path /unzip/checkpoints/ --port 7865 --device_id 0 --share true --torch_compile true --cpu_offload false --overlapped_decode true",
   "id": "24213017a353effb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
